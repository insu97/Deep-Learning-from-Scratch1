{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Xn0Xq8wbnnppK0pdgHm6Z0iXlDtje76X",
      "authorship_tag": "ABX9TyN3n3ebio9TxLVGRWgeio4c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/insu97/Deep-Learning-from-Scratch1/blob/main/7_%ED%95%A9%EC%84%B1%EA%B3%B1%EC%8B%A0%EA%B2%BD%EB%A7%9D(CNN).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeWABPjhE1zU",
        "outputId": "018c56f5-6fa0-4879-f66c-44cc61caee90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/drive/MyDrive/Deep-Learning-from-Scratch/01/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WT390Ex2FSKm",
        "outputId": "fc8b4ea3-78a3-4e3a-fe84-5c2bf58d9355"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Deep-Learning-from-Scratch/01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# im2col"
      ],
      "metadata": {
        "id": "OHCTZTSGH74z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
        "    \"\"\"다수의 이미지를 입력받아 2차원 배열로 변환한다(평탄화).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_data : 4차원 배열 형태의 입력 데이터(이미지 수, 채널 수, 높이, 너비)\n",
        "    filter_h : 필터의 높이\n",
        "    filter_w : 필터의 너비\n",
        "    stride : 스트라이드\n",
        "    pad : 패딩\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    col : 2차원 배열\n",
        "    \"\"\"\n",
        "    N, C, H, W = input_data.shape\n",
        "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
        "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
        "\n",
        "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
        "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
        "\n",
        "    for y in range(filter_h):\n",
        "        y_max = y + stride*out_h\n",
        "        for x in range(filter_w):\n",
        "            x_max = x + stride*out_w\n",
        "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
        "\n",
        "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
        "    return col"
      ],
      "metadata": {
        "id": "bbr29GyzH2j0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from common.util import im2col\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "LyD0C1-EFTkv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = np.random.rand(1, 3, 7, 7) # 데이터 수, 채널 수, 높이, 너비\n",
        "col1 = im2col(x1, 5, 5, stride=1, pad=0)  # input_data, 필터의 높이, 필터의 너비, 스트라이드, 패딩\n",
        "print(col1.shape)\n",
        "\n",
        "x2 = np.random.rand(10, 3, 7, 7)\n",
        "col2 = im2col(x2, 5, 5, stride=1, pad=0)\n",
        "print(col2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycRe0sdVFdgs",
        "outputId": "4e558deb-470c-404a-90c2-8c63e3b24715"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9, 75)\n",
            "(90, 75)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolution"
      ],
      "metadata": {
        "id": "EKkNFOKtH-5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Convolution:\n",
        "    def __init__(self, W, b, stride=1, pad=0):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "\n",
        "        # 중간 데이터（backward 시 사용）\n",
        "        self.x = None\n",
        "        self.col = None\n",
        "        self.col_W = None\n",
        "\n",
        "        # 가중치와 편향 매개변수의 기울기\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
        "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
        "\n",
        "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
        "        col_W = self.W.reshape(FN, -1).T\n",
        "\n",
        "        out = np.dot(col, col_W) + self.b\n",
        "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
        "\n",
        "        self.x = x\n",
        "        self.col = col\n",
        "        self.col_W = col_W\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
        "\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "        self.dW = np.dot(self.col.T, dout)\n",
        "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
        "\n",
        "        dcol = np.dot(dout, self.col_W.T)\n",
        "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
        "\n",
        "        return dx"
      ],
      "metadata": {
        "id": "SVVa3MjaIAin"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pooling"
      ],
      "metadata": {
        "id": "XMrqSUIhITkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Pooling:\n",
        "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
        "        self.pool_h = pool_h\n",
        "        self.pool_w = pool_w\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "\n",
        "        self.x = None\n",
        "        self.arg_max = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
        "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
        "\n",
        "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
        "\n",
        "        arg_max = np.argmax(col, axis=1)\n",
        "        out = np.max(col, axis=1)\n",
        "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
        "\n",
        "        self.x = x\n",
        "        self.arg_max = arg_max\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout = dout.transpose(0, 2, 3, 1)\n",
        "\n",
        "        pool_size = self.pool_h * self.pool_w\n",
        "        dmax = np.zeros((dout.size, pool_size))\n",
        "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
        "        dmax = dmax.reshape(dout.shape + (pool_size,))\n",
        "\n",
        "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
        "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "\n",
        "        return dx"
      ],
      "metadata": {
        "id": "Wj9q5-zkIqjN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SimpleConvNet"
      ],
      "metadata": {
        "id": "DRS63AgAJjbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "from common.layers import *"
      ],
      "metadata": {
        "id": "pDfv32KyKFZW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleConvNet:\n",
        "    \"\"\"단순한 합성곱 신경망\n",
        "\n",
        "    conv - relu - pool - affine - relu - affine - softmax\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_size : 입력 크기（MNIST의 경우엔 784）\n",
        "    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）\n",
        "    output_size : 출력 크기（MNIST의 경우엔 10）\n",
        "    activation : 활성화 함수 - 'relu' 혹은 'sigmoid'\n",
        "    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
        "        'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
        "        'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=(1, 28, 28),\n",
        "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
        "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
        "        filter_num = conv_param['filter_num']\n",
        "        filter_size = conv_param['filter_size']\n",
        "        filter_pad = conv_param['pad']\n",
        "        filter_stride = conv_param['stride']\n",
        "        input_size = input_dim[1]\n",
        "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
        "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
        "\n",
        "        # 가중치 초기화\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * \\\n",
        "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
        "        self.params['b1'] = np.zeros(filter_num)\n",
        "        self.params['W2'] = weight_init_std * \\\n",
        "                            np.random.randn(pool_output_size, hidden_size)\n",
        "        self.params['b2'] = np.zeros(hidden_size)\n",
        "        self.params['W3'] = weight_init_std * \\\n",
        "                            np.random.randn(hidden_size, output_size)\n",
        "        self.params['b3'] = np.zeros(output_size)\n",
        "\n",
        "        # 계층 생성\n",
        "        self.layers = OrderedDict()\n",
        "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
        "                                           conv_param['stride'], conv_param['pad'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
        "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
        "        self.layers['Relu2'] = Relu()\n",
        "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
        "\n",
        "        self.last_layer = SoftmaxWithLoss()\n",
        "\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def loss(self, x, t):\n",
        "        \"\"\"손실 함수를 구한다.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "        \"\"\"\n",
        "        y = self.predict(x)\n",
        "        return self.last_layer.forward(y, t)\n",
        "\n",
        "    def accuracy(self, x, t, batch_size=100):\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "\n",
        "        acc = 0.0\n",
        "\n",
        "        for i in range(int(x.shape[0] / batch_size)):\n",
        "            tx = x[i*batch_size:(i+1)*batch_size]\n",
        "            tt = t[i*batch_size:(i+1)*batch_size]\n",
        "            y = self.predict(tx)\n",
        "            y = np.argmax(y, axis=1)\n",
        "            acc += np.sum(y == tt)\n",
        "\n",
        "        return acc / x.shape[0]\n",
        "\n",
        "    def numerical_gradient(self, x, t):\n",
        "        \"\"\"기울기를 구한다（수치미분）.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
        "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
        "            grads['b1']、grads['b2']、... 각 층의 편향\n",
        "        \"\"\"\n",
        "        loss_w = lambda w: self.loss(x, t)\n",
        "\n",
        "        grads = {}\n",
        "        for idx in (1, 2, 3):\n",
        "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
        "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        \"\"\"기울기를 구한다(오차역전파법).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
        "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
        "            grads['b1']、grads['b2']、... 각 층의 편향\n",
        "        \"\"\"\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.last_layer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 결과 저장\n",
        "        grads = {}\n",
        "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
        "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def save_params(self, file_name=\"params.pkl\"):\n",
        "        params = {}\n",
        "        for key, val in self.params.items():\n",
        "            params[key] = val\n",
        "        with open(file_name, 'wb') as f:\n",
        "            pickle.dump(params, f)\n",
        "\n",
        "    def load_params(self, file_name=\"params.pkl\"):\n",
        "        with open(file_name, 'rb') as f:\n",
        "            params = pickle.load(f)\n",
        "        for key, val in params.items():\n",
        "            self.params[key] = val\n",
        "\n",
        "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
        "            self.layers[key].W = self.params['W' + str(i+1)]\n",
        "            self.layers[key].b = self.params['b' + str(i+1)]"
      ],
      "metadata": {
        "id": "kThCcpACIxVL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataset.mnist import load_mnist\n",
        "from common.trainer import Trainer"
      ],
      "metadata": {
        "id": "0MWA99kEJ3Dv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)"
      ],
      "metadata": {
        "id": "ak8y415wJlJi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, t_train = x_train[:5000], t_train[:5000]\n",
        "x_test, t_test = x_test[:1000], t_test[:1000]\n",
        "\n",
        "max_epochs = 20\n",
        "\n",
        "network = SimpleConvNet(input_dim=(1,28,28),\n",
        "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
        "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
        "\n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=max_epochs, mini_batch_size=100,\n",
        "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
        "                  evaluate_sample_num_per_epoch=1000)\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iYGRgWDJ1cd",
        "outputId": "576ba578-30d4-44ae-b09f-072f302f3680"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss:2.2998487401867376\n",
            "=== epoch:1, train acc:0.269, test acc:0.242 ===\n",
            "train loss:2.297251472037481\n",
            "train loss:2.2950531086075405\n",
            "train loss:2.2867525338290178\n",
            "train loss:2.2803431717224045\n",
            "train loss:2.264648174252993\n",
            "train loss:2.248797528588361\n",
            "train loss:2.241106647150505\n",
            "train loss:2.2166395670441865\n",
            "train loss:2.2080223927587586\n",
            "train loss:2.1362690783551233\n",
            "train loss:2.0998252492280383\n",
            "train loss:2.0827650182909365\n",
            "train loss:2.031640337396036\n",
            "train loss:1.9618968957707361\n",
            "train loss:1.899778492975318\n",
            "train loss:1.8096605428048536\n",
            "train loss:1.846723648866968\n",
            "train loss:1.6434954712673358\n",
            "train loss:1.6644533735605416\n",
            "train loss:1.5985906229172853\n",
            "train loss:1.4989751621040899\n",
            "train loss:1.3393637245327688\n",
            "train loss:1.429940967266581\n",
            "train loss:1.1584667924368592\n",
            "train loss:1.1017484729968698\n",
            "train loss:1.126547066872999\n",
            "train loss:1.1349797687473617\n",
            "train loss:0.9282708318644477\n",
            "train loss:0.9670863094765891\n",
            "train loss:0.9541703668846538\n",
            "train loss:0.7602215524273082\n",
            "train loss:0.9325336236237673\n",
            "train loss:0.6981096405672186\n",
            "train loss:0.8027731989642228\n",
            "train loss:0.8553947676568179\n",
            "train loss:0.662772538678982\n",
            "train loss:0.6237836793162347\n",
            "train loss:0.5877655191535137\n",
            "train loss:0.7086327817286248\n",
            "train loss:0.6583653639529585\n",
            "train loss:0.612141504753217\n",
            "train loss:0.621521025318437\n",
            "train loss:0.824916547842388\n",
            "train loss:0.6350337780486754\n",
            "train loss:0.6112474152505866\n",
            "train loss:0.645667896950081\n",
            "train loss:0.568517695878633\n",
            "train loss:0.35848340048519634\n",
            "train loss:0.6883636468214053\n",
            "train loss:0.4512891729929108\n",
            "=== epoch:2, train acc:0.771, test acc:0.775 ===\n",
            "train loss:0.6933467628462742\n",
            "train loss:0.6881220869592064\n",
            "train loss:0.6042392043529204\n",
            "train loss:0.5819336460185702\n",
            "train loss:0.5881535107730251\n",
            "train loss:0.5362955012419666\n",
            "train loss:0.405411196094889\n",
            "train loss:0.5231271428756278\n",
            "train loss:0.5362694470289212\n",
            "train loss:0.3651020816055381\n",
            "train loss:0.6180977575023435\n",
            "train loss:0.404683849077162\n",
            "train loss:0.41561530932986124\n",
            "train loss:0.4340034274463874\n",
            "train loss:0.4824589130289485\n",
            "train loss:0.3175914744679548\n",
            "train loss:0.49156712982200107\n",
            "train loss:0.2775924516780262\n",
            "train loss:0.4879809936091019\n",
            "train loss:0.7560871384985792\n",
            "train loss:0.4676833155868529\n",
            "train loss:0.4038324363267107\n",
            "train loss:0.437623144006765\n",
            "train loss:0.5275705965619479\n",
            "train loss:0.4519570794621214\n",
            "train loss:0.47320999780639456\n",
            "train loss:0.4119451920763303\n",
            "train loss:0.2987290074247073\n",
            "train loss:0.43724514477705867\n",
            "train loss:0.3061473620750889\n",
            "train loss:0.2126693128078846\n",
            "train loss:0.4001760437047983\n",
            "train loss:0.5697956622774654\n",
            "train loss:0.46774468059989194\n",
            "train loss:0.3727664984160903\n",
            "train loss:0.378446911707575\n",
            "train loss:0.396137253935874\n",
            "train loss:0.39143113686709907\n",
            "train loss:0.4209203160777352\n",
            "train loss:0.2648486429970467\n",
            "train loss:0.4822242767230946\n",
            "train loss:0.3609141701268896\n",
            "train loss:0.27038106223554015\n",
            "train loss:0.22232774176244116\n",
            "train loss:0.22805598220946716\n",
            "train loss:0.39267317875362906\n",
            "train loss:0.2774127948225701\n",
            "train loss:0.45705183727839516\n",
            "train loss:0.38622167221058445\n",
            "train loss:0.5494962697433425\n",
            "=== epoch:3, train acc:0.866, test acc:0.861 ===\n",
            "train loss:0.19123283916742767\n",
            "train loss:0.2570633183899508\n",
            "train loss:0.4490239816121918\n",
            "train loss:0.4389895042533772\n",
            "train loss:0.39715597316072876\n",
            "train loss:0.34763528375861097\n",
            "train loss:0.3340310870142389\n",
            "train loss:0.40360839202031834\n",
            "train loss:0.37263020317203976\n",
            "train loss:0.33684539603063846\n",
            "train loss:0.5214226104131978\n",
            "train loss:0.37370110319341554\n",
            "train loss:0.26625209290521795\n",
            "train loss:0.39365378355217423\n",
            "train loss:0.36811356353610686\n",
            "train loss:0.33985289688112225\n",
            "train loss:0.36063934959112176\n",
            "train loss:0.3625667597394882\n",
            "train loss:0.4226232445539888\n",
            "train loss:0.4318915859991209\n",
            "train loss:0.38996354044104875\n",
            "train loss:0.5954594079984777\n",
            "train loss:0.31890523829070494\n",
            "train loss:0.3000253279410593\n",
            "train loss:0.39127382690929385\n",
            "train loss:0.2793986788241816\n",
            "train loss:0.40225972619223177\n",
            "train loss:0.3017590981679166\n",
            "train loss:0.371490894391448\n",
            "train loss:0.41604381874142843\n",
            "train loss:0.24459801163900244\n",
            "train loss:0.23683411519319428\n",
            "train loss:0.34592323579641315\n",
            "train loss:0.304851502688443\n",
            "train loss:0.42581933478032885\n",
            "train loss:0.31954083509674325\n",
            "train loss:0.3553291078281039\n",
            "train loss:0.37435088418030477\n",
            "train loss:0.23743576020282586\n",
            "train loss:0.20686287202980075\n",
            "train loss:0.4244807153577599\n",
            "train loss:0.41497720516620035\n",
            "train loss:0.4644581803495639\n",
            "train loss:0.2827590753173886\n",
            "train loss:0.4915532690553177\n",
            "train loss:0.348929520158358\n",
            "train loss:0.22789895393885165\n",
            "train loss:0.3127182542423237\n",
            "train loss:0.5627212272047214\n",
            "train loss:0.21041852731807734\n",
            "=== epoch:4, train acc:0.893, test acc:0.877 ===\n",
            "train loss:0.30410700001177293\n",
            "train loss:0.21567250976563815\n",
            "train loss:0.23626851558592377\n",
            "train loss:0.3684345027394615\n",
            "train loss:0.2590236373401638\n",
            "train loss:0.2145226714676768\n",
            "train loss:0.36681403034704807\n",
            "train loss:0.4703982121137614\n",
            "train loss:0.23300202512621737\n",
            "train loss:0.29390456074030286\n",
            "train loss:0.18969056431817952\n",
            "train loss:0.26854161004130506\n",
            "train loss:0.31452533539678185\n",
            "train loss:0.18751063439607848\n",
            "train loss:0.3600532267327359\n",
            "train loss:0.35350664438838253\n",
            "train loss:0.2785483906480102\n",
            "train loss:0.470884545096225\n",
            "train loss:0.20221708645925932\n",
            "train loss:0.15530484037929482\n",
            "train loss:0.16082912601698018\n",
            "train loss:0.376107090490198\n",
            "train loss:0.3171856180075422\n",
            "train loss:0.17908193444108533\n",
            "train loss:0.2495192157093242\n",
            "train loss:0.2561045055726364\n",
            "train loss:0.3395425453500037\n",
            "train loss:0.3069251476529056\n",
            "train loss:0.2497389722854537\n",
            "train loss:0.22175609091822956\n",
            "train loss:0.3741516499436353\n",
            "train loss:0.2639266404879654\n",
            "train loss:0.2689506482119523\n",
            "train loss:0.46564985414056886\n",
            "train loss:0.37299828844423927\n",
            "train loss:0.28700918497229994\n",
            "train loss:0.36311813810709703\n",
            "train loss:0.4182758418487856\n",
            "train loss:0.19611936289955484\n",
            "train loss:0.24757784487231801\n",
            "train loss:0.264391992903154\n",
            "train loss:0.20659872398732684\n",
            "train loss:0.5674144009673604\n",
            "train loss:0.2683928267319094\n",
            "train loss:0.15685782676087992\n",
            "train loss:0.36082880841830517\n",
            "train loss:0.1752468828047892\n",
            "train loss:0.14435007150151913\n",
            "train loss:0.28704980898933125\n",
            "train loss:0.20556080930666273\n",
            "=== epoch:5, train acc:0.911, test acc:0.879 ===\n",
            "train loss:0.4112097167040571\n",
            "train loss:0.34359007799936253\n",
            "train loss:0.33461753821876383\n",
            "train loss:0.27801675170271756\n",
            "train loss:0.19727051823631897\n",
            "train loss:0.17263822893380648\n",
            "train loss:0.3476502237894609\n",
            "train loss:0.30250706746471384\n",
            "train loss:0.31458755098795815\n",
            "train loss:0.26653260170661996\n",
            "train loss:0.276198751049674\n",
            "train loss:0.1912207677090396\n",
            "train loss:0.22486744359681385\n",
            "train loss:0.33081164105552724\n",
            "train loss:0.2038877742010225\n",
            "train loss:0.2736159151574954\n",
            "train loss:0.23758345096598696\n",
            "train loss:0.22085328237287327\n",
            "train loss:0.2467184716932554\n",
            "train loss:0.15278144885282866\n",
            "train loss:0.1425205020233188\n",
            "train loss:0.22151884018972745\n",
            "train loss:0.27894353724028426\n",
            "train loss:0.26104373159385075\n",
            "train loss:0.24538186299146728\n",
            "train loss:0.306440600587547\n",
            "train loss:0.2384081358282009\n",
            "train loss:0.1672391674493279\n",
            "train loss:0.18219764433350666\n",
            "train loss:0.23904481189981425\n",
            "train loss:0.20573914081377226\n",
            "train loss:0.18636694926214986\n",
            "train loss:0.17862660249132223\n",
            "train loss:0.1505078036034322\n",
            "train loss:0.13962668634249176\n",
            "train loss:0.24363566692456526\n",
            "train loss:0.22698381982694257\n",
            "train loss:0.2885746383636153\n",
            "train loss:0.32731904320587335\n",
            "train loss:0.12348622482341591\n",
            "train loss:0.25188125231099556\n",
            "train loss:0.20989975955191556\n",
            "train loss:0.1558132494833542\n",
            "train loss:0.28661666293668153\n",
            "train loss:0.1760639935664788\n",
            "train loss:0.2982278477863063\n",
            "train loss:0.2658526501285862\n",
            "train loss:0.25531972874706876\n",
            "train loss:0.1834019148818558\n",
            "train loss:0.2994982742081413\n",
            "=== epoch:6, train acc:0.922, test acc:0.904 ===\n",
            "train loss:0.2693595851917413\n",
            "train loss:0.16396820285308752\n",
            "train loss:0.22899098553907532\n",
            "train loss:0.34741185853730017\n",
            "train loss:0.19526214313061996\n",
            "train loss:0.3610633488960416\n",
            "train loss:0.2181288238615354\n",
            "train loss:0.2923337207950727\n",
            "train loss:0.33423040914225993\n",
            "train loss:0.20282125014878713\n",
            "train loss:0.1314272853039248\n",
            "train loss:0.24211692291293807\n",
            "train loss:0.19572328208011164\n",
            "train loss:0.2608983073917427\n",
            "train loss:0.20620467674839818\n",
            "train loss:0.1777197801517593\n",
            "train loss:0.18474931589827953\n",
            "train loss:0.294831070801321\n",
            "train loss:0.12912412980753876\n",
            "train loss:0.352751935941389\n",
            "train loss:0.2283481158248486\n",
            "train loss:0.29424850626053267\n",
            "train loss:0.2914872272910796\n",
            "train loss:0.3942764324021825\n",
            "train loss:0.22616255653577957\n",
            "train loss:0.11659006102437891\n",
            "train loss:0.21350291923763223\n",
            "train loss:0.36876455667561714\n",
            "train loss:0.23340862781028904\n",
            "train loss:0.121379784659068\n",
            "train loss:0.22519194137343168\n",
            "train loss:0.13831652516486379\n",
            "train loss:0.2576743490681251\n",
            "train loss:0.17956760300701524\n",
            "train loss:0.188377150942819\n",
            "train loss:0.16210791543579414\n",
            "train loss:0.23878595176973844\n",
            "train loss:0.29940257564225\n",
            "train loss:0.2940545705421744\n",
            "train loss:0.18386051118685107\n",
            "train loss:0.12361560000464486\n",
            "train loss:0.3425424282529729\n",
            "train loss:0.07069010668243197\n",
            "train loss:0.26596974429842857\n",
            "train loss:0.1205480750679707\n",
            "train loss:0.23024793274597466\n",
            "train loss:0.2675628318347877\n",
            "train loss:0.12369465300503299\n",
            "train loss:0.1931474844944764\n",
            "train loss:0.11748884210468402\n",
            "=== epoch:7, train acc:0.931, test acc:0.914 ===\n",
            "train loss:0.22828672769242897\n",
            "train loss:0.15621679405131317\n",
            "train loss:0.14725402077036293\n",
            "train loss:0.21693715694699056\n",
            "train loss:0.1053947464920198\n",
            "train loss:0.15831108487278933\n",
            "train loss:0.20835057084205\n",
            "train loss:0.12115797076943992\n",
            "train loss:0.17521953164078777\n",
            "train loss:0.3334066808575456\n",
            "train loss:0.12727082118273314\n",
            "train loss:0.21888856380108426\n",
            "train loss:0.20222751870582908\n",
            "train loss:0.19942994276010725\n",
            "train loss:0.0955372405424656\n",
            "train loss:0.12178389948093152\n",
            "train loss:0.223127367312405\n",
            "train loss:0.1769769942668885\n",
            "train loss:0.30843474221550105\n",
            "train loss:0.12462926723294065\n",
            "train loss:0.29808761120814276\n",
            "train loss:0.23738324818317674\n",
            "train loss:0.1877874143581705\n",
            "train loss:0.28256517160457234\n",
            "train loss:0.128255131248167\n",
            "train loss:0.23179555840498012\n",
            "train loss:0.12293981575313659\n",
            "train loss:0.18591953269854206\n",
            "train loss:0.107799294794164\n",
            "train loss:0.2000849296122984\n",
            "train loss:0.2747411084323618\n",
            "train loss:0.1244267695486893\n",
            "train loss:0.14711830048377245\n",
            "train loss:0.20772735875450274\n",
            "train loss:0.16851396682199626\n",
            "train loss:0.12770702207590406\n",
            "train loss:0.17495150250753994\n",
            "train loss:0.15379462072473216\n",
            "train loss:0.1362790092987448\n",
            "train loss:0.21321048588135177\n",
            "train loss:0.2476290562237133\n",
            "train loss:0.22182917559549542\n",
            "train loss:0.2472211207035638\n",
            "train loss:0.15959731432942573\n",
            "train loss:0.33728651383181907\n",
            "train loss:0.2092070831427699\n",
            "train loss:0.0789607235654321\n",
            "train loss:0.17751515707491852\n",
            "train loss:0.18043342941975601\n",
            "train loss:0.08774405275444573\n",
            "=== epoch:8, train acc:0.94, test acc:0.91 ===\n",
            "train loss:0.1750669874641514\n",
            "train loss:0.1289858139264263\n",
            "train loss:0.14964710081657656\n",
            "train loss:0.08188593558242573\n",
            "train loss:0.2801854346681876\n",
            "train loss:0.1451111453085117\n",
            "train loss:0.10885256330354225\n",
            "train loss:0.191663970834385\n",
            "train loss:0.1336229777579622\n",
            "train loss:0.20595420151410834\n",
            "train loss:0.10851105686458838\n",
            "train loss:0.08711552215727457\n",
            "train loss:0.1751149377438793\n",
            "train loss:0.21987709700724875\n",
            "train loss:0.1613745297268113\n",
            "train loss:0.10154742885933751\n",
            "train loss:0.16451759335045196\n",
            "train loss:0.06408080527461599\n",
            "train loss:0.10715110990554687\n",
            "train loss:0.2457162684258305\n",
            "train loss:0.20845820092949452\n",
            "train loss:0.12110554042095795\n",
            "train loss:0.1373570316270779\n",
            "train loss:0.13431694164732272\n",
            "train loss:0.14379960050836285\n",
            "train loss:0.09972990525678128\n",
            "train loss:0.11898634056259816\n",
            "train loss:0.10740290869204178\n",
            "train loss:0.15101183315603206\n",
            "train loss:0.19822287957630266\n",
            "train loss:0.16278496948087537\n",
            "train loss:0.20550809235401346\n",
            "train loss:0.23742987772852533\n",
            "train loss:0.08625049004682939\n",
            "train loss:0.14771312586989288\n",
            "train loss:0.19075536451991887\n",
            "train loss:0.163117417569567\n",
            "train loss:0.15870777400561228\n",
            "train loss:0.15649983699947353\n",
            "train loss:0.059017006431739884\n",
            "train loss:0.15969033476218142\n",
            "train loss:0.13596426629100883\n",
            "train loss:0.12355508352264469\n",
            "train loss:0.12403315664478841\n",
            "train loss:0.13578512931592232\n",
            "train loss:0.11736247180799939\n",
            "train loss:0.17278850612619884\n",
            "train loss:0.13412298474347303\n",
            "train loss:0.15931000022501138\n",
            "train loss:0.17693022123395555\n",
            "=== epoch:9, train acc:0.951, test acc:0.929 ===\n",
            "train loss:0.16336485884634894\n",
            "train loss:0.1625996091850986\n",
            "train loss:0.1487968428606829\n",
            "train loss:0.081301978867256\n",
            "train loss:0.13487376639277152\n",
            "train loss:0.15015335497111498\n",
            "train loss:0.12351521219965696\n",
            "train loss:0.1215208996521649\n",
            "train loss:0.13526918014294706\n",
            "train loss:0.11350195246681656\n",
            "train loss:0.21618636393401341\n",
            "train loss:0.13821384057143946\n",
            "train loss:0.08410558296950707\n",
            "train loss:0.10091803082818643\n",
            "train loss:0.09353237871189657\n",
            "train loss:0.10993399937031693\n",
            "train loss:0.1683100135482509\n",
            "train loss:0.12896231874468228\n",
            "train loss:0.11881533518003956\n",
            "train loss:0.14920873887512576\n",
            "train loss:0.11508176643001095\n",
            "train loss:0.1744176677091672\n",
            "train loss:0.26482826756373207\n",
            "train loss:0.06399303465708508\n",
            "train loss:0.15455886885098644\n",
            "train loss:0.16593109796556818\n",
            "train loss:0.07302820510145001\n",
            "train loss:0.14609159396458954\n",
            "train loss:0.07056054235122856\n",
            "train loss:0.10734719664050697\n",
            "train loss:0.04959977512555903\n",
            "train loss:0.1024087536891936\n",
            "train loss:0.09169940777187437\n",
            "train loss:0.11648683351409941\n",
            "train loss:0.16322906969892734\n",
            "train loss:0.1456034104197453\n",
            "train loss:0.11108109311292091\n",
            "train loss:0.04259758611134026\n",
            "train loss:0.05021086552791323\n",
            "train loss:0.18026636347288755\n",
            "train loss:0.12044721096991796\n",
            "train loss:0.20611621114466572\n",
            "train loss:0.09221493219700667\n",
            "train loss:0.10815105206358724\n",
            "train loss:0.12097440714997015\n",
            "train loss:0.1529233028251466\n",
            "train loss:0.22889949140190022\n",
            "train loss:0.23540863204861232\n",
            "train loss:0.1630290584387261\n",
            "train loss:0.07612147616666862\n",
            "=== epoch:10, train acc:0.955, test acc:0.931 ===\n",
            "train loss:0.15617877641434835\n",
            "train loss:0.14260606759621758\n",
            "train loss:0.08874673458475948\n",
            "train loss:0.13970335080485138\n",
            "train loss:0.03510842739436481\n",
            "train loss:0.0826436747705727\n",
            "train loss:0.22326483025484137\n",
            "train loss:0.04367476697137476\n",
            "train loss:0.10404367156640343\n",
            "train loss:0.13340496398837012\n",
            "train loss:0.07357379747397409\n",
            "train loss:0.10263071082213773\n",
            "train loss:0.1713815015852938\n",
            "train loss:0.08586793590496133\n",
            "train loss:0.09188677101101625\n",
            "train loss:0.0450836272881393\n",
            "train loss:0.048263414044760394\n",
            "train loss:0.07468806738959217\n",
            "train loss:0.11830725374145352\n",
            "train loss:0.06328657151848316\n",
            "train loss:0.13259059902984316\n",
            "train loss:0.10763309262084801\n",
            "train loss:0.04264317203410953\n",
            "train loss:0.13627155692013168\n",
            "train loss:0.06589182279838296\n",
            "train loss:0.15081032818944723\n",
            "train loss:0.08068523893822023\n",
            "train loss:0.05197721039916501\n",
            "train loss:0.09383114371566414\n",
            "train loss:0.20047099318658593\n",
            "train loss:0.05112660611413697\n",
            "train loss:0.11099858632141277\n",
            "train loss:0.1928044175865945\n",
            "train loss:0.1968037728788443\n",
            "train loss:0.09659027557406871\n",
            "train loss:0.13716460199338704\n",
            "train loss:0.17337391397879656\n",
            "train loss:0.14885322130259837\n",
            "train loss:0.1276994169367776\n",
            "train loss:0.28122334391294457\n",
            "train loss:0.0894711190511933\n",
            "train loss:0.14160372864577447\n",
            "train loss:0.09661735123006875\n",
            "train loss:0.19108069814931333\n",
            "train loss:0.24311422827221563\n",
            "train loss:0.06989853248494145\n",
            "train loss:0.04302898356565411\n",
            "train loss:0.23938423601366923\n",
            "train loss:0.1515093360492633\n",
            "train loss:0.08644768468922216\n",
            "=== epoch:11, train acc:0.958, test acc:0.936 ===\n",
            "train loss:0.04850187314761046\n",
            "train loss:0.19780148255750574\n",
            "train loss:0.044110225053711434\n",
            "train loss:0.07253268584247252\n",
            "train loss:0.08103925125801431\n",
            "train loss:0.07692477030590748\n",
            "train loss:0.04184258314422208\n",
            "train loss:0.174868763261528\n",
            "train loss:0.10283752471478945\n",
            "train loss:0.07140990139505947\n",
            "train loss:0.09407285604851146\n",
            "train loss:0.1704817414366827\n",
            "train loss:0.12297411381163399\n",
            "train loss:0.07733296152772437\n",
            "train loss:0.15285254651892616\n",
            "train loss:0.12948592078989493\n",
            "train loss:0.1094468188889107\n",
            "train loss:0.09139125785523806\n",
            "train loss:0.050592087045094\n",
            "train loss:0.13243997391149961\n",
            "train loss:0.1444206721552176\n",
            "train loss:0.10693791100936528\n",
            "train loss:0.06702527008409523\n",
            "train loss:0.10186334706701926\n",
            "train loss:0.06571770472187381\n",
            "train loss:0.10330514875158948\n",
            "train loss:0.1288103560474337\n",
            "train loss:0.09465458500803248\n",
            "train loss:0.0827570041839258\n",
            "train loss:0.06481945774275656\n",
            "train loss:0.04493271394784462\n",
            "train loss:0.18889802621547724\n",
            "train loss:0.09763727991043539\n",
            "train loss:0.1376133704796456\n",
            "train loss:0.08052431086714722\n",
            "train loss:0.13026468593654825\n",
            "train loss:0.08740281776303274\n",
            "train loss:0.0751408398252793\n",
            "train loss:0.17331651376424356\n",
            "train loss:0.12363284311459706\n",
            "train loss:0.120657406316069\n",
            "train loss:0.1546973313671185\n",
            "train loss:0.06729115228932576\n",
            "train loss:0.1357038429480465\n",
            "train loss:0.06363925139750642\n",
            "train loss:0.1430424484120357\n",
            "train loss:0.19831004411594877\n",
            "train loss:0.1392102093373085\n",
            "train loss:0.08835606156466717\n",
            "train loss:0.09499279927144429\n",
            "=== epoch:12, train acc:0.96, test acc:0.934 ===\n",
            "train loss:0.08840924953273016\n",
            "train loss:0.14079929291664703\n",
            "train loss:0.11250005484734008\n",
            "train loss:0.1931798025219123\n",
            "train loss:0.12791571101289198\n",
            "train loss:0.07147084300400607\n",
            "train loss:0.058934962597951085\n",
            "train loss:0.032648569051459055\n",
            "train loss:0.035923496604267864\n",
            "train loss:0.08968200828747168\n",
            "train loss:0.21421104106592062\n",
            "train loss:0.07520399129524365\n",
            "train loss:0.059527140143893514\n",
            "train loss:0.0930722747426306\n",
            "train loss:0.09537203043371206\n",
            "train loss:0.08778139488991883\n",
            "train loss:0.07622052338728376\n",
            "train loss:0.0909939952864018\n",
            "train loss:0.10149508654621155\n",
            "train loss:0.10801343960168167\n",
            "train loss:0.07009526885555989\n",
            "train loss:0.07719941780648223\n",
            "train loss:0.07043465012236673\n",
            "train loss:0.04741664524589785\n",
            "train loss:0.10690760367584912\n",
            "train loss:0.14648224322791295\n",
            "train loss:0.08814096506862487\n",
            "train loss:0.07440569320264558\n",
            "train loss:0.07885126322820325\n",
            "train loss:0.021517834749358675\n",
            "train loss:0.0764327470264345\n",
            "train loss:0.06529785356032006\n",
            "train loss:0.07609458032347867\n",
            "train loss:0.03368535475796818\n",
            "train loss:0.1080314942765077\n",
            "train loss:0.07090677465654421\n",
            "train loss:0.05863620855576542\n",
            "train loss:0.08918149584619339\n",
            "train loss:0.23527178579891472\n",
            "train loss:0.16392706341898328\n",
            "train loss:0.09009281350336008\n",
            "train loss:0.07404134363621902\n",
            "train loss:0.05870592833813621\n",
            "train loss:0.0641302592204141\n",
            "train loss:0.08932884141273681\n",
            "train loss:0.0712024600751483\n",
            "train loss:0.12108278123030727\n",
            "train loss:0.03785205908839642\n",
            "train loss:0.03550303749930373\n",
            "train loss:0.15123730198079882\n",
            "=== epoch:13, train acc:0.967, test acc:0.942 ===\n",
            "train loss:0.05095501041093213\n",
            "train loss:0.06887503169659133\n",
            "train loss:0.057843539122815056\n",
            "train loss:0.09161590077813843\n",
            "train loss:0.05040177465951087\n",
            "train loss:0.08927083077067856\n",
            "train loss:0.11837383338109538\n",
            "train loss:0.04827151653564962\n",
            "train loss:0.10329649294266251\n",
            "train loss:0.0475551686900555\n",
            "train loss:0.06472505606575907\n",
            "train loss:0.06217718754669571\n",
            "train loss:0.10255391289662416\n",
            "train loss:0.14963127932084888\n",
            "train loss:0.07236681455496266\n",
            "train loss:0.118701777168003\n",
            "train loss:0.08012179517306672\n",
            "train loss:0.0474073807067866\n",
            "train loss:0.04141971132817048\n",
            "train loss:0.09447567889664907\n",
            "train loss:0.03797816340301388\n",
            "train loss:0.047324526264152504\n",
            "train loss:0.10783583936663843\n",
            "train loss:0.16818662984415464\n",
            "train loss:0.03579042633209435\n",
            "train loss:0.09185125863550318\n",
            "train loss:0.08923253553738225\n",
            "train loss:0.069953433449388\n",
            "train loss:0.07103210144651341\n",
            "train loss:0.075744232888313\n",
            "train loss:0.02873158987317787\n",
            "train loss:0.099918306720527\n",
            "train loss:0.04425723392029333\n",
            "train loss:0.06654739517941538\n",
            "train loss:0.09117669880715795\n",
            "train loss:0.049096913538774316\n",
            "train loss:0.07950288466855474\n",
            "train loss:0.09685237276029782\n",
            "train loss:0.03585487344413795\n",
            "train loss:0.05880915504022501\n",
            "train loss:0.08403912799841386\n",
            "train loss:0.08021448907990733\n",
            "train loss:0.12870405787124606\n",
            "train loss:0.06548709274291165\n",
            "train loss:0.07719511976413805\n",
            "train loss:0.047493211373428726\n",
            "train loss:0.04840538703452922\n",
            "train loss:0.08360422819394296\n",
            "train loss:0.05794980673530004\n",
            "train loss:0.13305008704918048\n",
            "=== epoch:14, train acc:0.971, test acc:0.941 ===\n",
            "train loss:0.05002093790445996\n",
            "train loss:0.03819638286178077\n",
            "train loss:0.05172423597251886\n",
            "train loss:0.06330112780270997\n",
            "train loss:0.03926105811468984\n",
            "train loss:0.1179419263753791\n",
            "train loss:0.04546646118215432\n",
            "train loss:0.04426016717583922\n",
            "train loss:0.04876877681009109\n",
            "train loss:0.049128816781842256\n",
            "train loss:0.12325477936899785\n",
            "train loss:0.0969786991482563\n",
            "train loss:0.045951663360024916\n",
            "train loss:0.040937710529354326\n",
            "train loss:0.06207831011075939\n",
            "train loss:0.05229863118945135\n",
            "train loss:0.16628258138731336\n",
            "train loss:0.12841511467712052\n",
            "train loss:0.09056899857384437\n",
            "train loss:0.02181074053862038\n",
            "train loss:0.03852784870446055\n",
            "train loss:0.09565597959640344\n",
            "train loss:0.044795174638348964\n",
            "train loss:0.0379808907269899\n",
            "train loss:0.11511629665713106\n",
            "train loss:0.04632755225883889\n",
            "train loss:0.08384513292150998\n",
            "train loss:0.0432344379058864\n",
            "train loss:0.04527769631563666\n",
            "train loss:0.0633862638846448\n",
            "train loss:0.03349270392043952\n",
            "train loss:0.04833091554194214\n",
            "train loss:0.13497120830950088\n",
            "train loss:0.08443880737878277\n",
            "train loss:0.03862438752879991\n",
            "train loss:0.06611650259951599\n",
            "train loss:0.04004593972640042\n",
            "train loss:0.03523203018227712\n",
            "train loss:0.08914593627683139\n",
            "train loss:0.10942512942933263\n",
            "train loss:0.025844310128416183\n",
            "train loss:0.048606405019126\n",
            "train loss:0.052602325990800426\n",
            "train loss:0.021388860807758956\n",
            "train loss:0.06087494606371898\n",
            "train loss:0.04169481019837863\n",
            "train loss:0.07648038208870991\n",
            "train loss:0.027489741969167262\n",
            "train loss:0.09592152436200109\n",
            "train loss:0.05567541704370291\n",
            "=== epoch:15, train acc:0.977, test acc:0.95 ===\n",
            "train loss:0.029717844487457984\n",
            "train loss:0.019674076006930975\n",
            "train loss:0.061239485353237415\n",
            "train loss:0.05382030506953508\n",
            "train loss:0.05385126190505582\n",
            "train loss:0.07750632198443017\n",
            "train loss:0.06436755361151426\n",
            "train loss:0.062290327343178256\n",
            "train loss:0.056370300046074524\n",
            "train loss:0.025831527566856326\n",
            "train loss:0.15599756848710444\n",
            "train loss:0.03812423331717518\n",
            "train loss:0.053087629883523266\n",
            "train loss:0.031523397669721225\n",
            "train loss:0.03232257330413369\n",
            "train loss:0.05807072319963285\n",
            "train loss:0.09504239625784415\n",
            "train loss:0.04175286194702081\n",
            "train loss:0.084475450535912\n",
            "train loss:0.05301210294578757\n",
            "train loss:0.07321219608244081\n",
            "train loss:0.06061111799215411\n",
            "train loss:0.05346638408238547\n",
            "train loss:0.023195521252022927\n",
            "train loss:0.10376664714704902\n",
            "train loss:0.057141786752894656\n",
            "train loss:0.06925326899566125\n",
            "train loss:0.0428557768707412\n",
            "train loss:0.060826021290600406\n",
            "train loss:0.15348263420829064\n",
            "train loss:0.02883536980491996\n",
            "train loss:0.019516717293048498\n",
            "train loss:0.03767907869793999\n",
            "train loss:0.03879845046100824\n",
            "train loss:0.0721648678122\n",
            "train loss:0.028991068457523594\n",
            "train loss:0.03135892234039511\n",
            "train loss:0.0397309060583117\n",
            "train loss:0.03833558895083818\n",
            "train loss:0.034349026667608945\n",
            "train loss:0.06012680624275027\n",
            "train loss:0.03121942754408581\n",
            "train loss:0.043618764933784276\n",
            "train loss:0.054675897241044547\n",
            "train loss:0.033850043487617325\n",
            "train loss:0.05055632019368541\n",
            "train loss:0.05497799821395884\n",
            "train loss:0.049218183902715575\n",
            "train loss:0.05831380613905004\n",
            "train loss:0.041875047568649786\n",
            "=== epoch:16, train acc:0.979, test acc:0.951 ===\n",
            "train loss:0.035670530649676215\n",
            "train loss:0.03687026927153935\n",
            "train loss:0.02361347923940604\n",
            "train loss:0.02680338833327163\n",
            "train loss:0.02949133606342643\n",
            "train loss:0.016593780000662958\n",
            "train loss:0.020578926340930427\n",
            "train loss:0.053411175917541624\n",
            "train loss:0.033974263161422054\n",
            "train loss:0.043665393590649006\n",
            "train loss:0.035912627133717547\n",
            "train loss:0.06457567307223\n",
            "train loss:0.022546808405075794\n",
            "train loss:0.02693672566609218\n",
            "train loss:0.02875541133881153\n",
            "train loss:0.034958771519337656\n",
            "train loss:0.02174526521572168\n",
            "train loss:0.032664240643972234\n",
            "train loss:0.044377459420061494\n",
            "train loss:0.03299846420977957\n",
            "train loss:0.21883935907286267\n",
            "train loss:0.07481073577238102\n",
            "train loss:0.03343522302427579\n",
            "train loss:0.05685700732341907\n",
            "train loss:0.02415019736138039\n",
            "train loss:0.017229759170188083\n",
            "train loss:0.04578764329294205\n",
            "train loss:0.030407713880838738\n",
            "train loss:0.040520590052288495\n",
            "train loss:0.05046144823393733\n",
            "train loss:0.05773457510375399\n",
            "train loss:0.05113030945778281\n",
            "train loss:0.03982398381923555\n",
            "train loss:0.023068626777269275\n",
            "train loss:0.022909405654796617\n",
            "train loss:0.04506005904549595\n",
            "train loss:0.0547912549619088\n",
            "train loss:0.11464976497645521\n",
            "train loss:0.01582063039785846\n",
            "train loss:0.07438608081664545\n",
            "train loss:0.05872994115687944\n",
            "train loss:0.070954268344122\n",
            "train loss:0.06234708888002242\n",
            "train loss:0.05491848774877652\n",
            "train loss:0.03307649380650857\n",
            "train loss:0.038565004999521195\n",
            "train loss:0.014245515795594918\n",
            "train loss:0.020498883450938078\n",
            "train loss:0.035852134602176185\n",
            "train loss:0.04882544557860103\n",
            "=== epoch:17, train acc:0.983, test acc:0.944 ===\n",
            "train loss:0.06495023562036399\n",
            "train loss:0.014434583975591173\n",
            "train loss:0.04028341064462802\n",
            "train loss:0.032511950001022805\n",
            "train loss:0.026092135601745073\n",
            "train loss:0.08323665153884743\n",
            "train loss:0.018417982548666598\n",
            "train loss:0.036268298110956484\n",
            "train loss:0.01459161005448233\n",
            "train loss:0.016763974093500122\n",
            "train loss:0.047272723168934945\n",
            "train loss:0.022290805339153667\n",
            "train loss:0.041707461836598736\n",
            "train loss:0.0221338079184151\n",
            "train loss:0.026533872668206757\n",
            "train loss:0.029586896392586378\n",
            "train loss:0.031566800723777516\n",
            "train loss:0.045471621628586076\n",
            "train loss:0.020140430613141514\n",
            "train loss:0.024655167829405644\n",
            "train loss:0.024465735520649325\n",
            "train loss:0.030147399770293883\n",
            "train loss:0.019025436369651923\n",
            "train loss:0.05001669605110102\n",
            "train loss:0.1412179547653441\n",
            "train loss:0.03072386582842588\n",
            "train loss:0.05982962741879194\n",
            "train loss:0.0843051076321009\n",
            "train loss:0.0324648827795005\n",
            "train loss:0.04402031580792461\n",
            "train loss:0.04829847442719191\n",
            "train loss:0.017487311267717857\n",
            "train loss:0.03890906763022836\n",
            "train loss:0.04050583431020424\n",
            "train loss:0.027124776154855857\n",
            "train loss:0.051394420463318304\n",
            "train loss:0.034709849280147184\n",
            "train loss:0.13699455135808614\n",
            "train loss:0.07099329529040033\n",
            "train loss:0.040906737717738624\n",
            "train loss:0.022879566011412486\n",
            "train loss:0.012262180400551232\n",
            "train loss:0.03902786990978814\n",
            "train loss:0.038529855386339254\n",
            "train loss:0.06554201387718689\n",
            "train loss:0.07915703855964812\n",
            "train loss:0.04426226726129648\n",
            "train loss:0.03220629899941327\n",
            "train loss:0.025231633318393242\n",
            "train loss:0.040286437513013\n",
            "=== epoch:18, train acc:0.985, test acc:0.955 ===\n",
            "train loss:0.019455949594997773\n",
            "train loss:0.03344929608651303\n",
            "train loss:0.024728718806684785\n",
            "train loss:0.050801311584547586\n",
            "train loss:0.044974706119038706\n",
            "train loss:0.0162723274215209\n",
            "train loss:0.102804751060486\n",
            "train loss:0.016333459001799\n",
            "train loss:0.09210368629094795\n",
            "train loss:0.04439720187683857\n",
            "train loss:0.030966866281053704\n",
            "train loss:0.05625829090378276\n",
            "train loss:0.026720091532646092\n",
            "train loss:0.026005676251837744\n",
            "train loss:0.028908491590615575\n",
            "train loss:0.027467141776410825\n",
            "train loss:0.03048474248304649\n",
            "train loss:0.05136337971296018\n",
            "train loss:0.024363949894123632\n",
            "train loss:0.03685746966101701\n",
            "train loss:0.07746030425717243\n",
            "train loss:0.016074901569650357\n",
            "train loss:0.03524962737765798\n",
            "train loss:0.04791868651861991\n",
            "train loss:0.01677123537782704\n",
            "train loss:0.0386945386096585\n",
            "train loss:0.04049430123486899\n",
            "train loss:0.03414263316249947\n",
            "train loss:0.021063955235117796\n",
            "train loss:0.09268593177599432\n",
            "train loss:0.05025926435150445\n",
            "train loss:0.014379362269252354\n",
            "train loss:0.026243477950903666\n",
            "train loss:0.023960295260133754\n",
            "train loss:0.020511790260894575\n",
            "train loss:0.0735385501424674\n",
            "train loss:0.09961619826747971\n",
            "train loss:0.03344206588029712\n",
            "train loss:0.03347020411346401\n",
            "train loss:0.02680165359776993\n",
            "train loss:0.03481229619677895\n",
            "train loss:0.015667364646789632\n",
            "train loss:0.02007899198736204\n",
            "train loss:0.017779434602232764\n",
            "train loss:0.11164339999088131\n",
            "train loss:0.02297148391249685\n",
            "train loss:0.04364914120591021\n",
            "train loss:0.03534070448730587\n",
            "train loss:0.0423442334254698\n",
            "train loss:0.019826019434104405\n",
            "=== epoch:19, train acc:0.994, test acc:0.953 ===\n",
            "train loss:0.03609068741570611\n",
            "train loss:0.042078312283881904\n",
            "train loss:0.01636757321695484\n",
            "train loss:0.012793534276955641\n",
            "train loss:0.04030948307057046\n",
            "train loss:0.02641961706189138\n",
            "train loss:0.0305845873385004\n",
            "train loss:0.05986298403472059\n",
            "train loss:0.022829673839279044\n",
            "train loss:0.03319958031979296\n",
            "train loss:0.04296780915975108\n",
            "train loss:0.020538264949263293\n",
            "train loss:0.01995365524232946\n",
            "train loss:0.030835940883189285\n",
            "train loss:0.03807135861846314\n",
            "train loss:0.05039118773500333\n",
            "train loss:0.0781596463059706\n",
            "train loss:0.023456697341542036\n",
            "train loss:0.019474183525805613\n",
            "train loss:0.012934737664010485\n",
            "train loss:0.03529196953799974\n",
            "train loss:0.0439299016335828\n",
            "train loss:0.03627077185979495\n",
            "train loss:0.06638951729818399\n",
            "train loss:0.013934567937298581\n",
            "train loss:0.023530980968094285\n",
            "train loss:0.0271899332181786\n",
            "train loss:0.06863851090707454\n",
            "train loss:0.014337642409450076\n",
            "train loss:0.022913998897941927\n",
            "train loss:0.02372103824085584\n",
            "train loss:0.012932644300961263\n",
            "train loss:0.022996872395451895\n",
            "train loss:0.03458923919447488\n",
            "train loss:0.020403403417959147\n",
            "train loss:0.03268419022088853\n",
            "train loss:0.02468212311545889\n",
            "train loss:0.02215643964741469\n",
            "train loss:0.053334395341211545\n",
            "train loss:0.040924181275662566\n",
            "train loss:0.02666506300639853\n",
            "train loss:0.03405441526239661\n",
            "train loss:0.0253810512302928\n",
            "train loss:0.021639937570379374\n",
            "train loss:0.020784384717678798\n",
            "train loss:0.03196146104634844\n",
            "train loss:0.016082766586790868\n",
            "train loss:0.018110215734642207\n",
            "train loss:0.07069464057449956\n",
            "train loss:0.012328367654579444\n",
            "=== epoch:20, train acc:0.986, test acc:0.952 ===\n",
            "train loss:0.015813587452578135\n",
            "train loss:0.018297197732302855\n",
            "train loss:0.015352809476390783\n",
            "train loss:0.019924346536299355\n",
            "train loss:0.0225609823860397\n",
            "train loss:0.011246946459826417\n",
            "train loss:0.060351734638235774\n",
            "train loss:0.009671935006043696\n",
            "train loss:0.019430206317569653\n",
            "train loss:0.030520442149622656\n",
            "train loss:0.025550321702990243\n",
            "train loss:0.01649266811949362\n",
            "train loss:0.031240713276907615\n",
            "train loss:0.039640022907639155\n",
            "train loss:0.035222838484623625\n",
            "train loss:0.015181752691869293\n",
            "train loss:0.01643855035425573\n",
            "train loss:0.05650496142506288\n",
            "train loss:0.009298148576227558\n",
            "train loss:0.026668902125830375\n",
            "train loss:0.021705190069157086\n",
            "train loss:0.023441160153094335\n",
            "train loss:0.006629133843839768\n",
            "train loss:0.04002355594513094\n",
            "train loss:0.01902268315913177\n",
            "train loss:0.0071340914361844796\n",
            "train loss:0.03798062418632163\n",
            "train loss:0.016668913154769288\n",
            "train loss:0.043408997715639365\n",
            "train loss:0.009131298016532601\n",
            "train loss:0.01860783578048583\n",
            "train loss:0.015220171130498698\n",
            "train loss:0.015252656994494069\n",
            "train loss:0.04766973748809497\n",
            "train loss:0.02305446603355911\n",
            "train loss:0.011989985468002735\n",
            "train loss:0.014094674591078452\n",
            "train loss:0.024894881597229555\n",
            "train loss:0.013018036306217482\n",
            "train loss:0.010292139144034495\n",
            "train loss:0.007448405011752618\n",
            "train loss:0.013880708472392186\n",
            "train loss:0.044235348437783324\n",
            "train loss:0.01364618260911809\n",
            "train loss:0.032514500767149686\n",
            "train loss:0.019585637971594766\n",
            "train loss:0.03143578501628751\n",
            "train loss:0.02214692669747963\n",
            "train loss:0.03719010939388931\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.954\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "N4sOAom6N3y7"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 매개변수 보존\n",
        "network.save_params(\"params.pkl\")\n",
        "print(\"Saved Network Parameters!\")\n",
        "\n",
        "# 그래프 그리기\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
        "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "SLipsV6mJ8p9",
        "outputId": "fedeb854-7d72-417b-960d-c77c1f21e4a7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved Network Parameters!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAG2CAYAAACDLKdOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABQT0lEQVR4nO3deXhTVf4/8PdN2iRNl3RfKW1ZFdlkqywOLoWi/FBcERcWl5lxcEZBHUBFRB3qhoOKI+qI6DgqylcdHRyURVABAdmUxSJYaAvdl3RLkzY5vz/ShoZuaZrkJun79Tx50tzce/O5vdS8PfeccyUhhAARERGRn1DIXQARERGRKzHcEBERkV9huCEiIiK/wnBDREREfoXhhoiIiPwKww0RERH5FYYbIiIi8isMN0RERORXGG6IiIjIrzDcEBERkV+RNdx8++23mDZtGhITEyFJEj777LNOt9m2bRtGjBgBtVqNfv36Ye3atW6vk4iIiHyHrOGmtrYWw4YNw6uvvurQ+jk5OZg6dSouv/xyHDx4EA888ADuvvtufPXVV26ulIiIiHyF5C03zpQkCZ9++immT5/e7joLFy7Ehg0bcPjwYduyW265BZWVldi4caMHqiQiIiJvFyB3AV2xa9cuZGRk2C3LzMzEAw880O42RqMRRqPR9tpisaC8vBxRUVGQJMldpRIREZELCSFQXV2NxMREKBQdX3jyqXBTWFiIuLg4u2VxcXGoqqqCwWBAUFBQq22ysrKwbNkyT5VIREREbpSXl4devXp1uI5PhRtnLF68GAsWLLC91uv16N27N/Ly8hAWFiZjZURE5IvMFoF9pypQUlOPmBANRqZGQKnwzisBQghsOVaEB9Yd6nTd1XeMwIR+MR6oyjlVVVVITk5GaGhop+v6VLiJj49HUVGR3bKioiKEhYW12WoDAGq1Gmq1utXysLAwhhsiIuqSjYcLsOyLoyjQ19uWJeg0WDptEKYMTpClJiEEiquNOFVai9NldThVZv9cY2yEQq3tdD9/+ugXxITmoFdEEHpFaJEUHtT0s/V1r4ggaAKVHjiijjnSpcSnws3YsWPx5Zdf2i3btGkTxo4dK1NFRETUU2w8XIB739uP80fhFOrrce97+/Ha7SPcFnDMFoECvcE+vDSFmdPltahvsLjkc0qqjSipNuJAbmWb70eHqFsFnqSIICRHBCEpXIsglfzhB5A53NTU1ODEiRO21zk5OTh48CAiIyPRu3dvLF68GGfOnMG7774LAPjjH/+IVatW4a9//SvuvPNObN26FR999BE2bNgg1yEQEVEXmS0Ce3LKUVxdj9hQDcakRXrtZZ1mZovAsi+Otgo2ACAASACWfXEUkwbFt3ksZouAsdEMY4MFxkaL9edGS9Nr87llTe9X1plwurzOFmbyyw0wmdsPMAoJ6BWhRUqUFqlRwbbn1GgtEnRByHhxOwr19W3WLwGI12nwxX0TUKCvR35FHfIrDMivqMOZSgPyKwzIK69DrcmM0hojSmuMOJhX2WYd0SEqJEVoMbyXDsuuHezAb9Y9ZA03P/74Iy6//HLb6+a+MbNnz8batWtRUFCA3Nxc2/tpaWnYsGED5s+fj5deegm9evXCP//5T2RmZnq8diIi6jpvvKzTUoPZgur6RlQZGlBV34AqQyOq6htwILfCrubzCQAF+npc9vw3UCqkprBigbHBGlwaLd2fdSVQKSE50j68pERpkRIVjKTwIKgC2h9BtHTaINz73n5ITbU2k1q8Hx2qRnSoGkN66VofnxDQGxpsocf6bP9zjbERpTUmlNaYoO6gFk/wmnluPKWqqgo6nQ56vZ59boiIPKi9yzrNX7DOXtYRQsBktrRqCak1NtrCyflhxfq6dYipM5m7fZydCVBIUAcooA5UWp8DFFAHKKEOPPdzsFqJ3pHW4NIcYhLDg7rVwuXOYCmEQJWhEXlNYUcTqMBlA2O7tc/zdeX7m+GGiMgH+dqlHVOjBROe3YriamO76wSrlLhmeCIazMKu1aPtSzgt33dNf5PzawkLCkSYJhBhQQEwmwX2t3MppqXHrr4QF6eEW8NKU1BRNQeYQAVUSgUClPK1avjav5uWuvL97VMdiomIyHsu7TT/33pJTT1Kqk0oqTHaOqSWVBvtXpfVGNvs79FSrcmMD/bkdbsudYACqgAFQtUBdgHF+hyIME3by0M11p9DNQGtAojZIjDh2a2d9luZOyHNq8OCUiFhbN8ouctwO4YbIiIf4u4RO/UNZmufk/oGVNaZ2g0tpU0/d9TJ1RlTBsdjWK9wW0vHuRaQji/jNP+sUircMvu8UiE51G/Fm4NNT8JwQ0TkIxwZsfPEF0cxLDkctUZzh31L2ltucuIST6gmADGhasSEqK3PzY8Wr3PL63Dve/s73dfssale27IwZXACXrt9RKtWs3gv6hBNVgw3RNQj+Urfg+ZLP4VV9dj6S1GnI3YK9fUYm7W1W58pSbBd0ukstESHqB2a2O2C+DAk6DSdXtYZkxbZrdrdbcrgBEwaFO8T/3Z6MoYbIupxvKnPSnmtCQX6ehTq61FQVY9CvcH2ulBfjwJ9PQwNXR/Bowtq0Z+kRd+S0M76nwQFIkQVAIWLv6z96bKOT/ZbqcwD6sraf18bBYQne64eN+NoKSJymq+0frTkruHIgDWs2EbyNE3IVlZrsgss54KMAUV6x/usRGitwSS3vK7Tdd+/Ox3j+kU7dQzu5i3BskepzANWjQQa2x+phgA1cN8+rw44HC1FRG7nC19SQggYGsyoNZpRZ2pEdX0jHv30cLt9VgDgwY8PYefJsqbhyGaYbGHFfuhxW7PNOtNfRZKsU9on6DSID9NYn3VBTc/W13FhGmgClQ6P2Env46WtCpV5mBJZhkmzInHkTBXK60yI1KpwUVIYlFIRUNno1V+uPquurONgA1jfryvzm98/ww0RdZm7RuwIIVBnMtt1cK2ub7CFE9uzyYw6Y9Nzy+XnvV/XYEZX26ZrjWa8u+t0l2s/nyRZhyRHalVNISXIFlbim4JMvE6D2FBNhzPLtuTTl3ZatB4oAQxtax0faD1wu0YTUPYrUF0AKNVAgMb6e2nv2Q0jw/wBww0RdYkjI3Ye/88RJEdqrSN2bKNx2h6d0zzsuPl9swumqW9LsEoJhUJCdX1jp+tOGhSLIUnhbQxBVrYeotzOcOUAheSWIck+O2LHH1oPXNlvxWIGKk4BxUeB4mPnnstOAJbO/43adBqAVJ3/3psd/Ddw6jtAEdD0ULb42YHXkvLcMnUIENnH8eNwMYYbInKYEAIbDxd0OmKnuNqIqS9/7/TnBCikpg6x1gnVglUBCFYroW3xrFXZvw5WKaFVNz2ft74mwBpsdp0sw8w3f+j08+8c38d7O4z6+6WdPW9YQ4LFbP2Stz3Of93WsvNea3RAWBIQltj0SALCEqzP2mhA0YWZgp3ttyIEoM+3DzDFR4HS40BjO39Hap11H5ZG6zqNxnPPDQbYtdmZjdaHg/mlQ3vecMFOmvQaDdy92XX76yKGGyJqU1mNEdlF1TheWI3sohr8WlSN7KJqh1o+ACBUrUR0qKbd2WDbXx4ITaB7JmIbkxbp28ORfeHSjsVivaRS/pv9o+ioY9sf/Ld762umCDwXdOzCT4vnkDhrSwTgeMvTr18DZlNTiGl6mKrbXj8gCIgZCMQOAmIvPPccltj+5SYh2g49jfXtLDMCpb8C373Q+e9kwFWAJszx8GhbZm69jlbe/zlguCHq4arqG/BrUTWOF9Ugu7Aax4usj9IaU5vrKyTAkStHb8wa7XWtH0qFhKwrw/HCp7sAtN1n5aErx3pnnxXAey7tmBuBqvwW4SXn3HNFTvstEo4YchMQGt/J5RAHL5EYKoCqM0DV2RbPZ4GaIsDSAFTmWh/tkZTWWsISgUCtY/VvWNB6mSIAiB7QFGCaQkzMBUBE6rnw5ChJApSB1oc61LFtzh50LNxctghIHN61erwUww1RD2EwmXGiuMYWXppbZc62c4lJkoDekVr0jw3FwPgQDIgLxcD4UPSO1OLKFdt9s/WjMg+XfX0VLlN3EBC+VgMDvLRTq6O9o499ARQddr7vRPMyc4O1X8j5rTAVp63hoD2SEohIsfa5aH4AwMZFndc+9j73f8GaG4Dqwtahp/nn6gLrszA3LTvj+L7DelnrbxlkIvta+76QxzDcEMnI1fPENJotOFtZj1NltThdXofTpbU4VVaLE8U1OF1e1+53Y4JOg/5xoRgYdy7E9IsNgVbV9n8ifLb1w1taPtpjMVu/dJtbFCpzgcrTLX528KaSjvxfencpVUBEWosAk3buWZdsbVlo6exB99fkKGWg9fx2dI4tZqC25FzgydsD7Hy5833f8m/va/3QRlkvV3bWX0jmS0muxHBDJBNn54kxNVqQV1GH3LI6a4hp8ZxXXofGDq4ZRWgDMTA+FAPjQjGg6bl/XCh0QYHtbtOKr7d+OKK2FKgrPzfqpKuXDtpjMVtbBVqGlZbhRZ/fcYuIo1LGA6rgDvpKWBzrmCspmlpgWoSY5kATlui634s3UjRdkgqNB5JGWgObI+HGG4UnW/th9aAZihluiGTQ2TwxL90yHBckhOFUqX14OVVWi7OVhg77vKgCFEiJ1CIlKhipUVqkRAejT3QwBsSFIjpE1f2Out7U+iEEYKqxflZdmTWQ2H4ua728utCx/f77BvvXisDO5xtpbxiuoeK88NJJh2xFgLVDa3hvIDyl6bnpYaoB3r+58/ozl7P1gOx11lLlZxhuiDyss3liAOAvHx7scB9alfJceGl67h2lRWpUMOLDNC6/L5DLmM8f5dHO6I6Wzw0GwHB+aGnx2tx2x+fuOW+KPEsDYGpof9RLVygCAF2vFqGlKcDokq3PoQmAsp3/NHvTpZ2u6oGtByQfhhsiN9LXNeBUWa1dy8vhM/oO54lpFhSoQP+40HPhJVKL1OhgpERpEROidstQaZf6YKb1uWVQEV2/AaRDAjTWeUu0kdYvSLtHi2W1pcD/3dn5/n6/DYgb3PUg1tbQXHWY9dJOc5gJTfDvyzkd8eXWA7Y8+RSGG6JuEEKgrNaE02W1OFVaZ30uq7N25i2rRWWd8/0nnrl+KK69OMmF1XaTEEDBIWDfO46tX3224/eVKgcv7WjaCC0tXgdFAioHh+l2peVDGQAoQ6wzrXoLfsHKhy1PPoXhhqgTQggUVRmbWl+s4aVlZ94aY8d9KOLC1EiJCkZKU8uLsdGM9Vt+QITU/iWOChGK2DCNqw+l60y1wG/bgeMbrZOTVRc4vu21/wDiB7cdWJTqrs0OS1b8gpWXL7c89TAMN+TzXDmc+vxZeZvnhOloVl5JAhJ1QUhp0f8lJSoYqdHWS0nnD6c2V+Ri3o4HoUb7rTpGBCIg8nIAMvwfeMVpa5A5vhHI+c46tXuzQC2QOAI47cCtFeIuAhKGua9OZ/hDywe/YIk6xXBDPs3Z4dTNs/JmF1oDTHZhNX4tbn9WXqVCQq+IoFadeFOigtErIgiaQMf7UCgN5VB2EGwAWIOPoRyI6O3wfp1mbgTy91rDzPGvgJJj9u+H9wYGTAEGZAIpE4CSX4A3Jrq/LndgywdRj8BwQz6rs+HUr90+AhMHxOJEcY21NaYpxBwvqm63Q2/zrLwD4s7NBTMgLgRp0cFQB/hRJ1BDBXBiizXMnNhkfd1MUgDJl1jDzIBM6zTxLTsv+3rrB1s+iPweww35JEeGU897/wDMHUwIk6DTYECcNbw4MitvtwhhnWOl9Dhwcqtj2/zfXdaRNRpd5w912LnntvqyCGH97ObWmdwf7EcuacKB/pOA/plAvyutHXbbw9YPIvJyDDfkk/bklHU6nLo52EQGqzAgLqR7s/I6qtFovfdO6fGmx69Nzye6PkdK2Qnro0ukc0HH9ggDio5YZ8JtKeZCYMBk6yWnXmPan1ulLWz9ICIvxnBDPqO6vgE7TpRh+/ESbDzs2Kidp6cPxu2XpLi+mNqyFgGmKcSU/Wq9yaCwtL2NpLTeBTgkDsjd2flnXPU8EBwF1OsdezTWAxCAUW996M/bn1IFpP3O2jozYLK1FiIiP8RwQ17LYhE4WlCF7cdLsD27BPtzKzq8b1Jb+sa4YI6SspNA9pdASfa5lhhDefvrq8OA6P5A9IAWzwOs9+QJUFnnWnGkQ27ymK5Nod9QDxirWgSeSqC+yvocEm8NNt40ZwsRkZsw3JBXKasx4rtfS/Ht8RJ8+2tJq9FLfaKD8bsBMbi0XzQe+exnFFcZ2+x3IwGI11mHhTulwQAc+wLY/y5w6ru219H1Pi/ENP0cEmffAddTAjXWR0is5z+biMiLMNyQrBrNFhzMq7S2zhwvwc9n9BAt0opWpcS4vtGYODAGE/vHoHfUuZlol1ksuPe9/effBQjNsWLptEFdn++m4CdroPn5I2vrR/Me+1wGJKefCzBR/RyfFfd8vj7aiIjIy0lCiK618/u4qqoq6HQ66PV6hIWFyV1Oj3S20oBvm8LM9ydKW02Qd2FCGCYOiMHEATEYmRIBVUD7M9k6O8+NnXo98PPHwP5/AQUHzy3X9QYuvh0YfqvrO89W5nG0ERFRF3Tl+5vhhtxOCIEfT1fg6yOF2H68BMeLauzeD9cG4tL+Mfhd/2j8bkAM4rp42wGnZigWAji9EzjwL+DIZ0CjwbpcEQhc+P+AEbOAtMt4iwAiIi/Rle9vXpYit6k1NuKzg2fw7s7TyC46NwxaIQHDk8Pxu6bWmaG9wp2+XQJgnT14bF8HL+FUFwGHPrCGmpbDrGMuBEbcAQy9xTpCiYiIfBbDDbncbyU1+NcPp7H+x3xUN91UMihQiauGxOOKC2IxoV80wrWq7n+Qo5d2zI3AyS3WvjTZ/zs3eV1gMDDkBuDiWUCvUfJ0AiYiIpdjuCGXMFsEvvmlGO/sOoXvfi21LU+N0uKOsam4cWQv106aV5kHrBrZcadcpQoYMQf45Qv7u1n3Gm297HTRdYA61HU1ERGRV2C4oW6pqDVh3Y95eO+H08ivsPZbkSTgioGxmDUuFZf2i4aiG5ec2lVX1nGwAQCzCdj7hvXnoEhg2EzrpafYC11fDxEReQ2GG3LKz/l6vLPrFL44dBbGRuuMvOHaQMwYlYzbL0lBcqSTw6Q7Y7EAZiNgdPBWBr1GA2PnAQOvtg6vJiIiv8dwQw4zNprx5c8FeGfnaRzMq7QtH5wUhlljU3HNsERoAtu4c3ZlHnB6B6DPt7a2NNbbP5uNbS9v69lsar3/jlz9Qtdm+SUiIp/HcEOdDqU+W2nAv3efxod78lBWaw0XgUoJU4ckYNa4VFycHA6pZWfcylzg1PfAqR3W2X3Pv2EjERGRGzHc9GSVedj5czZe//Y3u9scRIeo8PtL+0Cti8U/f2rA10cL0XxLp/gwDW5L741bxvRGTGjTZZ6K09aWmVPfN4WZXPvPkZTW1pPYC4GAIOvloQBN00PtxLPaep+nf17pmd8TERH5FIabnqoyD+aXR2CcxYRxANCyO0oDgK1AvQjET8YVsCAal/SJxOyxqZg0KA4BVXnAifVNYeZ7QN9WmLkYSJ0ApF4K9E53/agkBf/pEhFR2/gN0UOZa0uhtHTcf0UjNeDWwUG4Oj0VfWoOACfeATZ/D+jz7FeUlEDSCGuYSZngnjBDRETkIIabHurImSoMdWC93+cvhupEqf1CRQCQ2BRmUscDyZcA6hC31Nku3nySiIjawXDTQ5XXOTbqSFVfag0zSSOBlPHWQJOc7vkwc77wZOC+fbz5JBERtcJw00NFOnj7g99GL0OfSfcAqmA3V+SE8GSGFyIiaoW3PO6hLkpy7I7oKcMnemewISIiagfDTQ+ldPAmkY6uR0RE5C0YbnqoQy1mGCYiIvInDDc9UHV9A17akg0hOlmRo42IiMgHsUNxD5S14TDuNqyFpATMEX2gvG512zeV5GgjIiLyQQw3Pcy27GLE7H8F4wKPwhyghfK2j4Do/nKXRURE5DIMNz2Ivq4BH338b6wK+AQAoLzmJQYbIiLyO+xz04Os+PQ7PNGwEgpJoHHY7cDQm+UuiYiIyOUYbnqIrw+fwaRfliBWqoQhfCACpj4vd0lERERuwXDTA5TXmnDykydxqfIwTAoNgm77F6DSyl0WERGRWzDc9ADvfPAefm9eZ30xdQUQM1DegoiIiNyI4cbPfb33MGbmPQmlJFDR/0aoRt4ud0lERERuxXDjx0qqDAje8CfESxUoC0pDxE0vy10SERGR2zHc+CkhBL57+1GMxyHUQ42wO97jDTCJiKhHYLjxU99u/hzXlL8NACif+DQCEwfLXBEREZFnMNz4oaLCM7hgxwMIkCz4JfYqJF52j9wlEREReQzDjZ8RFjMK1s5BHMqRr+yFfnPfACRJ7rKIiIg8huHGzxz66GkMr9+DehEIyw1rEBAUJndJREREHiV7uHn11VeRmpoKjUaD9PR07Nmzp8P1V65ciYEDByIoKAjJycmYP38+6uvrPVStdys+sh2Dj60EAOy9YCF6D0qXtyAiIiIZyBpu1q1bhwULFmDp0qXYv38/hg0bhszMTBQXF7e5/vvvv49FixZh6dKlOHbsGN566y2sW7cOjzzyiIcr9z6WmjIoPrkLAZIF36snYtzND8pdEhERkSxkDTcvvvgi7rnnHsydOxeDBg3C6tWrodVqsWbNmjbX37lzJ8aPH49bb70VqampmDx5MmbOnNlpa4/fEwJn3pmLaHMJTot4JM9+A0ql7I1yREREspDtG9BkMmHfvn3IyMg4V4xCgYyMDOzatavNbcaNG4d9+/bZwsxvv/2GL7/8EldffXW7n2M0GlFVVWX38Dflm/+O5JLtMIpA/DR2JVIS4+UuiYiISDYBcn1waWkpzGYz4uLi7JbHxcXhl19+aXObW2+9FaWlpZgwYQKEEGhsbMQf//jHDi9LZWVlYdmyZS6t3ZuYc/cibMfTAID3wv+AuZOnyFwRERGRvHzq2sW2bduwfPly/OMf/8D+/fvxySefYMOGDXjqqafa3Wbx4sXQ6/W2R15engcrdjNDBerevwMBMGOjuASTZz0ChYLDvomIqGeTreUmOjoaSqUSRUVFdsuLiooQH9/2ZZUlS5bgjjvuwN133w0AGDJkCGpra/H73/8ejz76KBSK1llNrVZDrVa7/gDkJgRqPvoDQusLcNoSi5rMF5EcxdsrEBERydZyo1KpMHLkSGzZssW2zGKxYMuWLRg7dmyb29TV1bUKMEqlEoD1Xko9iXnXawjJ+QpGEYA1iUtxw7hBcpdERETkFWRruQGABQsWYPbs2Rg1ahTGjBmDlStXora2FnPnzgUAzJo1C0lJScjKygIATJs2DS+++CIuvvhipKen48SJE1iyZAmmTZtmCzk9wpl9wKYlAIAXpTvwx5k3QOIsxERERABkDjczZsxASUkJHn/8cRQWFmL48OHYuHGjrZNxbm6uXUvNY489BkmS8Nhjj+HMmTOIiYnBtGnT8Le//U2uQ/A8QyVMH86BSjTif+bRGHDtg0jQBcldFRERkdeQRA+7nlNVVQWdTge9Xo+wMB+7NYEQsKybBcUvnyPPEoPnUv+Jl+dOZKsNERH5va58f8vackNdtPefUPzyOUxCiUXKBfj7TZcw2BAREZ3Hp4aC92hFR2DZaJ3P55nGW3HL9OmIDdXIXBQREZH3YbjxEY2HPobCYsK35iEounAupg1LlLskIiIir8Rw4yPycrIBAAcCh+Op64bIXA0REZH3YrjxEZLeOrNyWt8LEBmskrkaIiIi78Vw4yNCjYUAAG1MmsyVEBEReTeGG19gbkC4uQwAoEvsI3MxRERE3o3hxgdYKvOhhAVGEYj4hGS5yyEiIvJqDDc+oLLwNwBAAaKQEK6VuRoiIiLvxnDjA/QF1nBTqoxFgJKnjIiIqCP8pvQBhtJTAIBqTYK8hRAREfkAhhsfICqsw8BNwUkyV0JEROT9GG58QGDNGQCAFNFb5kqIiIi8H8ONDwitLwAABMWkyFwJERGR92O48XYWCyLNxQAAXTznuCEiIuoMw42Xa6wuhhoNsAgJsUkMN0RERJ1huPFyZWdOAgCKEYHY8FCZqyEiIvJ+DDderrLQGm5KlbFQKCSZqyEiIvJ+DDderr74FADOcUNEROQohhsvZ6nMBQAYQzjHDRERkSMYbrxc8xw3Ch1vmElEROQIhhsvF1xfCADQcI4bIiIihzDceLmoxiIAQFh8X5krISIi8g0MN16svqYCYagFAMT0YrghIiJyBMONFyvJPwEAqBQhiIyIlLkaIiIi38Bw48UqC34DAJQoYyFJnOOGiIjIEQw3XszQNMdNjTpe3kKIiIh8CMONFzNzjhsiIqIuY7jxYs1z3CCcc9wQERE5iuHGi4UYCgAAmuhUeQshIiLyIQw3Xqx5jpvQ+DSZKyEiIvIdDDdeqra2FjGoAADEJveXuRoiIiLfwXDjpYryTwIADFAhNIKjpYiIiBzFcOOlKs5aw02pIhbgHDdEREQOY7jxUnUlpwAAVZzjhoiIqEsYbryUpSIPAGDiHDdERERdwnDjpQKq860/6DjHDRERUVcw3Hip4HrrHDdqznFDRETUJQw3XkgIgaiGpjlu4jjHDRERUVcw3HghfZ0RcSgFAMT06idzNURERL6F4cYLFZ45DZVkRiMU0ET2krscIiIin8Jw44Wa57gpV0QBygCZqyEiIvItDDde6NwcNwnyFkJEROSDGG68kLk8FwBgDE6UuRIiIiLfw3DjhZrnuBGc44aIiKjLGG68kG2Om6gUmSshIiLyPQw3XkYIgQjbHDd9ZK6GiIjI9zDceJmS6nokNM1xE5nUV+ZqiIiIfA/DjZcpKCpCqGQAAKgie8tcDRERke9huPEy5WdOAAD0Ch2g0spcDRERke9huPEyhqY5bvSqeHkLISIi8lEMN16mkXPcEBERdQvDjZdRVp8BwDluiIiInMVw42WCDWcBACrOcUNEROQUhhsvYrZwjhsiIqLuYrjxIoVV9UiUSgAA4QkMN0RERM5guPEiZ4rLESNVAQCUEZzjhoiIyBkMN16kvOA3AEC9pAGCImSuhoiIyDcx3HiR2uJTAJrmuJEkeYshIiLyUQw3XqSx/DQAoD44SeZKiIiIfBfDjRcJqM4HAFjCeslcCRERke9iuPEi2jrOcUNERNRdDDdewtRoQWSjdY6bkLg0mashIiLyXbKHm1dffRWpqanQaDRIT0/Hnj17Oly/srIS8+bNQ0JCAtRqNQYMGIAvv/zSQ9W6z9lKA5KkUgBAGMMNERGR0wLk/PB169ZhwYIFWL16NdLT07Fy5UpkZmYiOzsbsbGxrdY3mUyYNGkSYmNjsX79eiQlJeH06dMIDw/3fPEull9WjUtQDgCQwjnHDRERkbNkDTcvvvgi7rnnHsydOxcAsHr1amzYsAFr1qzBokWLWq2/Zs0alJeXY+fOnQgMDAQApKamerJktykrzEWAZEEjlAgIjZe7HCIiIp8l22Upk8mEffv2ISMj41wxCgUyMjKwa9euNrf5/PPPMXbsWMybNw9xcXEYPHgwli9fDrPZ3O7nGI1GVFVV2T28UU2RdQK/KlUcoFDKXA0REZHvki3clJaWwmw2Iy4uzm55XFwcCgsL29zmt99+w/r162E2m/Hll19iyZIlWLFiBZ5++ul2PycrKws6nc72SE5OdulxuEpjRS4AoF6bKHMlREREvk32DsVdYbFYEBsbizfeeAMjR47EjBkz8Oijj2L16tXtbrN48WLo9XrbIy8vz4MVO05Z1TTHjY5z3BAREXWHbH1uoqOjoVQqUVRUZLe8qKgI8fFt9zlJSEhAYGAglMpzl20uvPBCFBYWwmQyQaVStdpGrVZDrVa7tng3sM1xE8k5boiIiLpDtpYblUqFkSNHYsuWLbZlFosFW7ZswdixY9vcZvz48Thx4gQsFott2fHjx5GQkNBmsPEVBpMZUbY5blLlLYaIiMjHyXpZasGCBXjzzTfxzjvv4NixY7j33ntRW1trGz01a9YsLF682Lb+vffei/Lyctx///04fvw4NmzYgOXLl2PevHlyHYJL5FfUIVEqAwAExXCOGyIiou6QdSj4jBkzUFJSgscffxyFhYUYPnw4Nm7caOtknJubC4XiXP5KTk7GV199hfnz52Po0KFISkrC/fffj4ULF8p1CC6RX16H9KYJ/DjHDRERUfdIQgghdxGeVFVVBZ1OB71ej7CwMLnLAQCs234AM765zPri0SIgUCNrPURERN6mK9/fPjVayl/VFOUAAKoDohhsiIiIusmpcPPNN9+4uo4eraHsNACgXpsgcyVERES+z6lwM2XKFPTt2xdPP/20184b40uU1U1z3IR55wSDREREvsSpcHPmzBncd999WL9+Pfr06YPMzEx89NFHMJlMrq6vR9DUFQAAAqPYmZiIiKi7nAo30dHRmD9/Pg4ePIjdu3djwIAB+NOf/oTExET85S9/waFDh1xdp9+qqm9AjLkYABAcy2HgRERE3dXtDsUjRozA4sWLcd9996GmpgZr1qzByJEjcemll+LIkSOuqNGv5ZXXIUkqAQCoo1PlLYaIiMgPOB1uGhoasH79elx99dVISUnBV199hVWrVqGoqAgnTpxASkoKbrrpJlfW6pfyKwxIaprjBjr2uSEiIuoupybx+/Of/4wPPvgAQgjccccdeO655zB48GDb+8HBwXjhhReQmMg7XHemoKQMkVKN9QVvmklERNRtToWbo0eP4pVXXsH111/f7k0po6OjOWTcAbVNc9zUK4OhCQqXtxgiIiI/4FS4aXmzy3Z3HBCAiRMnOrP7HsXUNMeNISgRnL6PiIio+5zqc5OVlYU1a9a0Wr5mzRo8++yz3S6qJ1FUNc9xw0tSREREruBUuHn99ddxwQUXtFp+0UUXYfXq1d0uqqcQQiDIcBYAEBCVInM1RERE/sGpcFNYWIiEhNa3CoiJiUFBQUG3i+opymtNiLVY57jRxqTKWwwREZGfcCrcJCcnY8eOHa2W79ixgyOkuiCvxTDwwEi23BAREbmCUx2K77nnHjzwwANoaGjAFVdcAcDayfivf/0rHnzwQZcW6M/yK+owgnPcEBERuZRT4ebhhx9GWVkZ/vSnP9nuJ6XRaLBw4UIsXrzYpQX6szOlVbgKFdYX4Qw3REREruBUuJEkCc8++yyWLFmCY8eOISgoCP379293zhtqm74kF0pJoFEKREBwrNzlEBER+QWnwk2zkJAQjB492lW19DgNpc1z3CQgVNHt23wRERERuhFufvzxR3z00UfIzc21XZpq9sknn3S7sJ5AUZUHADBzjhsiIiKXcaq54MMPP8S4ceNw7NgxfPrpp2hoaMCRI0ewdetW6HQ6V9folywWgaC6pjluOFKKiIjIZZwKN8uXL8ff//53fPHFF1CpVHjppZfwyy+/4Oabb0bv3r1dXaNfKq42Il6UAACCohluiIiIXMWpcHPy5ElMnToVAKBSqVBbWwtJkjB//ny88cYbLi3QX+VV1NnmuFFGMBASERG5ilPhJiIiAtXV1QCApKQkHD58GABQWVmJuro611Xnx/Ir6pAolVlfcBg4ERGRyzgVbn73u99h06ZNAICbbroJ999/P+655x7MnDkTV155pUsL9Fd5ZedabjiBHxERkes4NVpq1apVqK+vBwA8+uijCAwMxM6dO3HDDTfgsccec2mB/qqi5Aw0UgMEJEhhSXKXQ0RE5De6HG4aGxvx3//+F5mZmQAAhUKBRYsWubwwf2cqs85xU6+JRVCASuZqiIiI/EeXL0sFBATgj3/8o63lhpxjm+MmlK02REREruRUn5sxY8bg4MGDLi6l52g0W6CtKwAABERypBQREZErOdXn5k9/+hMWLFiAvLw8jBw5EsHBwXbvDx061CXF+asCfT3iYe1MrI5KlbcYIiIiP+NUuLnlllsAAH/5y19syyRJghACkiTBbDa7pjo/lVdeh15NI6WkCI6UIiIiciWnwk1OTo6r6+hR8isMGGwbBs7LUkRERK7kVLhJSeHtArojr6IOmZL11gucwI+IiMi1nAo37777bofvz5o1y6lieorikhLopKaZnHW8IzgREZErORVu7r//frvXDQ0NqKurg0qlglarZbjpRPMcN6ZAHVTqUJmrISIi8i9ODQWvqKiwe9TU1CA7OxsTJkzABx984Ooa/Y5UlQ+Ac9wQERG5g1Phpi39+/fHM88806pVh+zVN5ihNVjnuFFGsu8SERGRq7ks3ADW2YvPnj3ryl36nTOVBtsw8EBO4EdERORyTvW5+fzzz+1eCyFQUFCAVatWYfz48S4pzF/lldchqWmklBTOcENERORqToWb6dOn272WJAkxMTG44oorsGLFClfU5bfyKwwYZJvjhiOliIiIXM2pcGOxWFxdR4+RV1GHSc3hhnPcEBERuZxL+9xQ5wpL9YiTKq0vODsxERGRyzkVbm644QY8++yzrZY/99xzuOmmm7pdlD8zlOcBAMxKDRAcLXM1RERE/sepcPPtt9/i6quvbrX8qquuwrffftvtovyZpLeGm8aQJECSZK6GiIjI/zgVbmpqaqBSqVotDwwMRFVVVbeL8le1xkaEGZvnuOElKSIiIndwKtwMGTIE69ata7X8ww8/xKBBg7pdlL/Kq6hDUlNn4oAIdiYmIiJyB6dGSy1ZsgTXX389Tp48iSuuuAIAsGXLFnzwwQf4+OOPXVqgP8krNyAJzcPA2XJDRETkDk6Fm2nTpuGzzz7D8uXLsX79egQFBWHo0KHYvHkzJk6c6Ooa/UZ+RR0GSGXWFxwGTkRE5BZOhRsAmDp1KqZOnerKWvxeXrkBl9sm8GO4ISIicgen+tzs3bsXu3fvbrV89+7d+PHHH7tdlL/KL69BAltuiIiI3MqpcDNv3jzk5eW1Wn7mzBnMmzev20X5q5qyM1BLjRCSAghNlLscIiIiv+RUuDl69ChGjBjRavnFF1+Mo0ePdrsofySEgEKfDwBoDI4HlE5fESQiIqIOOBVu1Go1ioqKWi0vKChAQAC/tNuiNzQgoqEQAKCM4EgpIiIid3Eq3EyePBmLFy+GXq+3LausrMQjjzyCSZMmuaw4f5JXbrDNcaMIZ7ghIiJyF6eaWV544QX87ne/Q0pKCi6++GIAwMGDBxEXF4d//etfLi3QX+RV1CGRnYmJiIjczqlwk5SUhJ9++gn//ve/cejQIQQFBWHu3LmYOXMmAgMDXV2jX8ivqENfDgMnIiJyO6c7yAQHB2PChAno3bs3TCYTAOB///sfAOCaa65xTXV+JK/cgN81hxu23BAREbmNU+Hmt99+w3XXXYeff/4ZkiRBCAGpxR2uzWazywr0F3nltbY+N2y5ISIich+nOhTff//9SEtLQ3FxMbRaLQ4fPozt27dj1KhR2LZtm4tL9A8V5SUIlQzWF7pe8hZDRETkx5xqudm1axe2bt2K6OhoKBQKKJVKTJgwAVlZWfjLX/6CAwcOuLpOnyaEACrzgADArImEUhUsd0lERER+y6mWG7PZjNDQUABAdHQ0zp49CwBISUlBdna266rzEyU1RsRYSgAAEue4ISIiciunWm4GDx6MQ4cOIS0tDenp6XjuueegUqnwxhtvoE+fPq6u0efZz3HD/jZERETu5FS4eeyxx1BbWwsAePLJJ/H//t//w6WXXoqoqCisW7fOpQX6g/yKuhadidlyQ0RE5E5OhZvMzEzbz/369cMvv/yC8vJyRERE2I2aIqv8CgNSJetlKXYmJiIici+n+ty0JTIy0ulg8+qrryI1NRUajQbp6enYs2ePQ9t9+OGHkCQJ06dPd+pzPSWvvEXLDS9LERERuZXLwo2z1q1bhwULFmDp0qXYv38/hg0bhszMTBQXF3e43alTp/DQQw/h0ksv9VClzsuzuyzFcENEROROsoebF198Effccw/mzp2LQYMGYfXq1dBqtVizZk2725jNZtx2221YtmyZT3RgLiqrRIxUZX3Bm2YSERG5lazhxmQyYd++fcjIyLAtUygUyMjIwK5du9rd7sknn0RsbCzuuuuuTj/DaDSiqqrK7uFJZosA9GcAAJZALRAU4dHPJyIi6mlkDTelpaUwm82Ii4uzWx4XF4fCwsI2t/n+++/x1ltv4c0333ToM7KysqDT6WyP5GTPXhYqrKpHPJrmuAnvDbDDNRERkVvJflmqK6qrq3HHHXfgzTffRHR0tEPbLF68GHq93vbIy8tzc5X2WnYmltjfhoiIyO2cviu4K0RHR0OpVKKoqMhueVFREeLj41utf/LkSZw6dQrTpk2zLbNYLACAgIAAZGdno2/fvnbbqNVqqNVqN1TvGGu44TBwIiIiT5G15UalUmHkyJHYsmWLbZnFYsGWLVswduzYVutfcMEF+Pnnn3Hw4EHb45prrsHll1+OgwcPevySkyPyKwwcBk5ERORBsrbcAMCCBQswe/ZsjBo1CmPGjMHKlStRW1uLuXPnAgBmzZqFpKQkZGVlQaPRYPDgwXbbh4eHA0Cr5d4ir6IOY6Uy6wvOTkxEROR2soebGTNmoKSkBI8//jgKCwsxfPhwbNy40dbJODc3FwqFT3UNspNfbkAS2HJDRETkKZIQQshdhCdVVVVBp9NBr9cjLCzM7Z83fvnX2Ga8BYGSGZh/FNAluf0ziYiI/E1Xvr99t0nEB5gaLRDVhQiUzBCKACC0dSdpIiIici2GGzc6W2lAYtMcNwhLBBRKeQsiIiLqARhu3KjlPaUkdiYmIiLyCIYbN8orNyCpeaQUOxMTERF5BMONG+XzbuBEREQex3DjRnmcwI+IiMjjGG7cqOV9pdhyQ0RE5BkMN26UX17LcENERORhDDduYjCZYa4tg1YyWhfwpplEREQewXDjJvkVdUhsHikVHAsEauQtiIiIqIdguHGTvIo69GJnYiIiIo9juHET6xw37G9DRETkaQw3bmI3xw1bboiIiDyG4cZN2HJDREQkD4YbN7HeV6rpppkMN0RERB7DcOMmdhP48bIUERGRxzDcuIHe0ICG+hpESjXWBWy5ISIi8hiGGzewm+NGHQYEhctaDxERUU/CcOMGeeWGc3PcsNWGiIjIoxhu3MBuGDhvu0BERORRDDdukF9hODdSip2JiYiIPIrhxg3sRkrxshQREZFHMdy4QV7LDsVsuSEiIvIohhsXE0I0XZZqbrnpLW9BREREPQzDjYuV15pgMhkRj3LrArbcEBEReRTDjYvlVRgQL1VAKQlAqQKCY+UuiYiIqEdhuHGxvPI6JKHpklRYEqDgr5iIiMiT+M3rYhwGTkREJC+GGxezGynFzsREREQex3DjYrwbOBERkbwYblzMfhg4ww0REZGnMdy4kMUicMYu3PC+UkRERJ7GcONCxdVGmMxmXpYiIiKSEcONC+VV1CEaVdBIDQAkIIwtN0RERJ7GcONCeeV1SGxutQmNBwJU8hZERETUAzHcuBA7ExMREcmP4cZFzBaBfafLbeHGwnBDREQkC4YbF9h4uAATnt2K7cdLbeHm37+YsfFwgcyVERER9TwMN9208XAB7n1vPwr09QCAXk3h5nh9BO59bz8DDhERkYcx3HSD2SKw7IujEC2WNbfc5ItoAMCyL47CbBFtbE1ERETuwHDTDXtyym0tNs2aR0udEdEQAAr09diTUy5DdURERD0Tw003FFfbB5tgGBAu1QKwhpv21iMiIiL3YbjphthQjd3r5ktSlSIYtQhqdz0iIiJyH4abbhiTFokEnQZS0+vmcHO2qdVGApCg02BMWqQ8BRIREfVADDfdoFRIWDptEABrkElq0d+mOfAsnTYISoXU9g6IiIjI5RhuumnK4AS8dvsIxOs0tmHg+SIa8ToNXrt9BKYMTpC5QiIiop4lQO4CfF5lHqZElmHSrEhUfVEJFAKZQ5Iwa0IklFIRUNnIu4MTERF5EMNNd1TmAatGAo1GKAFENC1OPLYGOLbG+iJADdy3jwGHiIjIQ3hZqjvqyoBGY8frNBqt6xEREZFHMNwQERGRX2G4ISIiIr/CcENERER+heGGiIiI/ArDDREREfkVhhsiIiLyKww33aGNss5j05EAtXU9IiIi8ghO4tcd4cnWCfo6msdGG8UJ/IiIiDyI4aa7wpMZXoiIiLwIL0sRERGRX2G4ISIiIr/CcENERER+heGGiIiI/ArDDREREfkVrwg3r776KlJTU6HRaJCeno49e/a0u+6bb76JSy+9FBEREYiIiEBGRkaH6xMREVHPInu4WbduHRYsWIClS5di//79GDZsGDIzM1FcXNzm+tu2bcPMmTPxzTffYNeuXUhOTsbkyZNx5swZD1dORERE3kgSQgg5C0hPT8fo0aOxatUqAIDFYkFycjL+/Oc/Y9GiRZ1ubzabERERgVWrVmHWrFmdrl9VVQWdTge9Xo+wsLBu109ERETu15Xvb1lbbkwmE/bt24eMjAzbMoVCgYyMDOzatcuhfdTV1aGhoQGRkZFtvm80GlFVVWX3ICIiIv8la7gpLS2F2WxGXFyc3fK4uDgUFhY6tI+FCxciMTHRLiC1lJWVBZ1OZ3skJ3M2YSIiIn8me5+b7njmmWfw4Ycf4tNPP4VGo2lzncWLF0Ov19seeXl5Hq6SiIiIPEnWe0tFR0dDqVSiqKjIbnlRURHi4+M73PaFF17AM888g82bN2Po0KHtrqdWq6FWd3LnbiIiIvIbsrbcqFQqjBw5Elu2bLEts1gs2LJlC8aOHdvuds899xyeeuopbNy4EaNGjfJEqUREROQjZL8r+IIFCzB79myMGjUKY8aMwcqVK1FbW4u5c+cCAGbNmoWkpCRkZWUBAJ599lk8/vjjeP/995GammrrmxMSEoKQkBDZjoOIiIi8g+zhZsaMGSgpKcHjjz+OwsJCDB8+HBs3brR1Ms7NzYVCca6B6bXXXoPJZMKNN95ot5+lS5fiiSee8GTpRERE5IVkn+fG0zjPDRERke/xmXluiIiIiFyN4YaIiIj8CsMNERER+RWGGyIiIvIrDDdERETkVxhuiIiIyK8w3BAREZFfYbghIiIiv8JwQ0RERH6F4YaIiIj8CsMNERER+RWGGyIiIvIrDDdERETkVwLkLoCIiMifmM1mNDQ0yF2GT1KpVFAout/uwnBDRETkAkIIFBYWorKyUu5SfJZCoUBaWhpUKlW39sNwQ0RE5ALNwSY2NhZarRaSJMldkk+xWCw4e/YsCgoK0Lt37279/hhuiIiIuslsNtuCTVRUlNzl+KyYmBicPXsWjY2NCAwMdHo/7FBMRETUTc19bLRarcyV+Lbmy1Fms7lb+2G4ISIichFeiuoeV/3+GG6IiIjIrzDcEBEReQmzRWDXyTL85+AZ7DpZBrNFyF1Sl6SmpmLlypVyl8EOxURERN5g4+ECLPviKAr09bZlCToNlk4bhCmDE9z2uZdddhmGDx/uklCyd+9eBAcHd7+obmLLDRERkcw2Hi7Ave/ttws2AFCor8e97+3HxsMFMlVmnb+nsbHRoXVjYmK8olM1ww0REZGLCSFQZ2p06FFd34Clnx9BWxegmpc98flRVNc3OLQ/IRy/lDVnzhxs374dL730EiRJgiRJWLt2LSRJwv/+9z+MHDkSarUa33//PU6ePIlrr70WcXFxCAkJwejRo7F582a7/Z1/WUqSJPzzn//EddddB61Wi/79++Pzzz/v+i+0i3hZioiIyMUMDWYMevwrl+xLACisqseQJ752aP2jT2ZCq3Ls6/2ll17C8ePHMXjwYDz55JMAgCNHjgAAFi1ahBdeeAF9+vRBREQE8vLycPXVV+Nvf/sb1Go13n33XUybNg3Z2dno3bt3u5+xbNkyPPfcc3j++efxyiuv4LbbbsPp06cRGRnpUI3OYMsNERFRD6XT6aBSqaDVahEfH4/4+HgolUoAwJNPPolJkyahb9++iIyMxLBhw/CHP/wBgwcPRv/+/fHUU0+hb9++nbbEzJkzBzNnzkS/fv2wfPly1NTUYM+ePW49LrbcEBERuVhQoBJHn8x0aN09OeWY8/beTtdbO3c0xqR13toRFKh06HM7M2rUKLvXNTU1eOKJJ7BhwwYUFBSgsbERBoMBubm5He5n6NChtp+Dg4MRFhaG4uJil9TYHoYbIiIiF5MkyeFLQ5f2j0GCToNCfX2b/W4kAPE6DS7tHwOlwnOTBJ4/6umhhx7Cpk2b8MILL6Bfv34ICgrCjTfeCJPJ1OF+zr+NgiRJsFgsLq+3JV6WIiIikpFSIWHptEEArEGmpebXS6cNcluwUalUDt3uYMeOHZgzZw6uu+46DBkyBPHx8Th16pRbauouhhsiIiKZTRmcgNduH4F4ncZuebxOg9duH+HWeW5SU1Oxe/dunDp1CqWlpe22qvTv3x+ffPIJDh48iEOHDuHWW291ewuMs3hZioiIyAtMGZyASYPisSenHMXV9YgN1WBMWqTbL0U99NBDmD17NgYNGgSDwYC33367zfVefPFF3HnnnRg3bhyio6OxcOFCVFVVubU2Z0miKwPi/UBVVRV0Oh30ej3CwsLkLoeIiPxAfX09cnJykJaWBo1G0/kG1KaOfo9d+f7mZSkiIiLyKww3RERE5FcYboiIiMivMNwQERGRX2G4ISIiIr/CcENERER+heGGiIiI/ArDDREREfkVhhsiIiLyK7z9AhERkdwq84C6svbf10YB4cmeq8fHMdwQERHJqTIPWDUSaDS2v06AGrhvn1sCzmWXXYbhw4dj5cqVLtnfnDlzUFlZic8++8wl+3MGL0sRERHJqa6s42ADWN/vqGWH7DDcEBERuZoQgKnWsUejwbF9Nhoc218X7oc9Z84cbN++HS+99BIkSYIkSTh16hQOHz6Mq666CiEhIYiLi8Mdd9yB0tJS23br16/HkCFDEBQUhKioKGRkZKC2thZPPPEE3nnnHfznP/+x7W/btm1d/OV1Hy9LERERuVpDHbA80bX7XDPFsfUeOQuogh1a9aWXXsLx48cxePBgPPnkkwCAwMBAjBkzBnfffTf+/ve/w2AwYOHChbj55puxdetWFBQUYObMmXjuuedw3XXXobq6Gt999x2EEHjooYdw7NgxVFVV4e233wYAREZGOnW43cFwQ0RE1EPpdDqoVCpotVrEx8cDAJ5++mlcfPHFWL58uW29NWvWIDk5GcePH0dNTQ0aGxtx/fXXIyUlBQAwZMgQ27pBQUEwGo22/cmB4YaIiMjVArXWFhRHFP7kWKvMnRuB+KGOfXY3HDp0CN988w1CQkJavXfy5ElMnjwZV155JYYMGYLMzExMnjwZN954IyIiIrr1ua7EcENERORqkuTwpSEEBDm+nqP77IaamhpMmzYNzz77bKv3EhISoFQqsWnTJuzcuRNff/01XnnlFTz66KPYvXs30tLS3F6fI9ihmIiIqAdTqVQwm8221yNGjMCRI0eQmpqKfv362T2Cg63hSpIkjB8/HsuWLcOBAwegUqnw6aeftrk/OTDcEBERyUkbZZ3HpiMBaut6bpCamordu3fj1KlTKC0txbx581BeXo6ZM2di7969OHnyJL766ivMnTsXZrMZu3fvxvLly/Hjjz8iNzcXn3zyCUpKSnDhhRfa9vfTTz8hOzsbpaWlaGhocEvdHeFlKSIiIjmFJ1sn6JNphuKHHnoIs2fPxqBBg2AwGJCTk4MdO3Zg4cKFmDx5MoxGI1JSUjBlyhQoFAqEhYXh22+/xcqVK1FVVYWUlBSsWLECV111FQDgnnvuwbZt2zBq1CjU1NTgm2++wWWXXeaW2tsjCdGFAfF+oKqqCjqdDnq9HmFhYXKXQ0REfqC+vh45OTlIS0uDRqORuxyf1dHvsSvf37wsRURERH6F4YaIiIj8CsMNERER+RWGGyIiIvIrDDdEREQu0sPG6Licq35/DDdERETdFBgYCACoq6uTuRLfZjKZAABKpbJb++E8N0RERN2kVCoRHh6O4uJiAIBWq4UkSTJX5VssFgtKSkqg1WoRENC9eMJwQ0RE5ALNd8FuDjjUdQqFAr179+52MGS4ISIicgFJkpCQkIDY2FhZbjngD1QqFRSK7veYYbghIiJyIaVS2e0+I9Q9XtGh+NVXX0Vqaio0Gg3S09OxZ8+eDtf/+OOPccEFF0Cj0WDIkCH48ssvPVQpEREReTvZw826deuwYMECLF26FPv378ewYcOQmZnZ7jXLnTt3YubMmbjrrrtw4MABTJ8+HdOnT8fhw4c9XDkRERF5I9lvnJmeno7Ro0dj1apVAKy9pZOTk/HnP/8ZixYtarX+jBkzUFtbi//+97+2ZZdccgmGDx+O1atXd/p5vHEmERGR7+nK97esfW5MJhP27duHxYsX25YpFApkZGRg165dbW6za9cuLFiwwG5ZZmYmPvvsszbXNxqNMBqNttd6vR6A9ZdEREREvqH5e9uRNhlZw01paSnMZjPi4uLslsfFxeGXX35pc5vCwsI21y8sLGxz/aysLCxbtqzV8uTkZCerJiIiIrlUV1dDp9N1uI7fj5ZavHixXUuPxWJBeXk5oqKiXD7BUlVVFZKTk5GXl+f3l7x4rP6rJx0vj9V/9aTj7SnHKoRAdXU1EhMTO11X1nATHR0NpVKJoqIiu+VFRUW2yZDOFx8f36X11Wo11Gq13bLw8HDni3ZAWFiYX/8Da4nH6r960vHyWP1XTzrennCsnbXYNJN1tJRKpcLIkSOxZcsW2zKLxYItW7Zg7NixbW4zduxYu/UBYNOmTe2uT0RERD2L7JelFixYgNmzZ2PUqFEYM2YMVq5cidraWsydOxcAMGvWLCQlJSErKwsAcP/992PixIlYsWIFpk6dig8//BA//vgj3njjDTkPg4iIiLyE7OFmxowZKCkpweOPP47CwkIMHz4cGzdutHUazs3NtZuKedy4cXj//ffx2GOP4ZFHHkH//v3x2WefYfDgwXIdgo1arcbSpUtbXQbzRzxW/9WTjpfH6r960vH2pGN1lOzz3BARERG5kuwzFBMRERG5EsMNERER+RWGGyIiIvIrDDdERETkVxhuuujVV19FamoqNBoN0tPTsWfPng7X//jjj3HBBRdAo9FgyJAh+PLLLz1UqfOysrIwevRohIaGIjY2FtOnT0d2dnaH26xduxaSJNk9NBqNhyrunieeeKJV7RdccEGH2/jieQWA1NTUVscqSRLmzZvX5vq+dF6//fZbTJs2DYmJiZAkqdX95oQQePzxx5GQkICgoCBkZGTg119/7XS/Xf2b95SOjrehoQELFy7EkCFDEBwcjMTERMyaNQtnz57tcJ/O/C14Qmfnds6cOa3qnjJlSqf79cZz29mxtvX3K0kSnn/++Xb36a3n1Z0Ybrpg3bp1WLBgAZYuXYr9+/dj2LBhyMzMRHFxcZvr79y5EzNnzsRdd92FAwcOYPr06Zg+fToOHz7s4cq7Zvv27Zg3bx5++OEHbNq0CQ0NDZg8eTJqa2s73C4sLAwFBQW2x+nTpz1UcfdddNFFdrV///337a7rq+cVAPbu3Wt3nJs2bQIA3HTTTe1u4yvntba2FsOGDcOrr77a5vvPPfccXn75ZaxevRq7d+9GcHAwMjMzUV9f3+4+u/o370kdHW9dXR3279+PJUuWYP/+/fjkk0+QnZ2Na665ptP9duVvwVM6O7cAMGXKFLu6P/jggw736a3ntrNjbXmMBQUFWLNmDSRJwg033NDhfr3xvLqVIIeNGTNGzJs3z/babDaLxMREkZWV1eb6N998s5g6dardsvT0dPGHP/zBrXW6WnFxsQAgtm/f3u46b7/9ttDpdJ4ryoWWLl0qhg0b5vD6/nJehRDi/vvvF3379hUWi6XN9331vAIQn376qe21xWIR8fHx4vnnn7ctq6ysFGq1WnzwwQft7qerf/NyOf9427Jnzx4BQJw+fbrddbr6tyCHto519uzZ4tprr+3Sfnzh3DpyXq+99lpxxRVXdLiOL5xXV2PLjYNMJhP27duHjIwM2zKFQoGMjAzs2rWrzW127dpltz4AZGZmtru+t9Lr9QCAyMjIDterqalBSkoKkpOTce211+LIkSOeKM8lfv31VyQmJqJPnz647bbbkJub2+66/nJeTSYT3nvvPdx5550d3kTWl89rs5ycHBQWFtqdN51Oh/T09HbPmzN/895Mr9dDkqRO763Xlb8Fb7Jt2zbExsZi4MCBuPfee1FWVtbuuv5ybouKirBhwwbcddddna7rq+fVWQw3DiotLYXZbLbNnNwsLi4OhYWFbW5TWFjYpfW9kcViwQMPPIDx48d3OAv0wIEDsWbNGvznP//Be++9B4vFgnHjxiE/P9+D1TonPT0da9euxcaNG/Haa68hJycHl156Kaqrq9tc3x/OKwB89tlnqKysxJw5c9pdx5fPa0vN56Yr582Zv3lvVV9fj4ULF2LmzJkd3lixq38L3mLKlCl49913sWXLFjz77LPYvn07rrrqKpjN5jbX95dz+8477yA0NBTXX399h+v56nntDtlvv0Debd68eTh8+HCn12fHjh1rd/PScePG4cILL8Trr7+Op556yt1ldstVV11l+3no0KFIT09HSkoKPvroI4f+j8hXvfXWW7jqqquQmJjY7jq+fF7JqqGhATfffDOEEHjttdc6XNdX/xZuueUW289DhgzB0KFD0bdvX2zbtg1XXnmljJW515o1a3Dbbbd12snfV89rd7DlxkHR0dFQKpUoKiqyW15UVIT4+Pg2t4mPj+/S+t7mvvvuw3//+19888036NWrV5e2DQwMxMUXX4wTJ064qTr3CQ8Px4ABA9qt3dfPKwCcPn0amzdvxt13392l7Xz1vDafm66cN2f+5r1Nc7A5ffo0Nm3a1GGrTVs6+1vwVn369EF0dHS7dfvDuf3uu++QnZ3d5b9hwHfPa1cw3DhIpVJh5MiR2LJli22ZxWLBli1b7P7PtqWxY8farQ8AmzZtand9byGEwH333YdPP/0UW7duRVpaWpf3YTab8fPPPyMhIcENFbpXTU0NTp482W7tvnpeW3r77bcRGxuLqVOndmk7Xz2vaWlpiI+PtztvVVVV2L17d7vnzZm/eW/SHGx+/fVXbN68GVFRUV3eR2d/C94qPz8fZWVl7dbt6+cWsLa8jhw5EsOGDevytr56XrtE7h7NvuTDDz8UarVarF27Vhw9elT8/ve/F+Hh4aKwsFAIIcQdd9whFi1aZFt/x44dIiAgQLzwwgvi2LFjYunSpSIwMFD8/PPPch2CQ+69916h0+nEtm3bREFBge1RV1dnW+f8Y122bJn46quvxMmTJ8W+ffvELbfcIjQajThy5Igch9AlDz74oNi2bZvIyckRO3bsEBkZGSI6OloUFxcLIfznvDYzm82id+/eYuHCha3e8+XzWl1dLQ4cOCAOHDggAIgXX3xRHDhwwDY66JlnnhHh4eHiP//5j/jpp5/EtddeK9LS0oTBYLDt44orrhCvvPKK7XVnf/Ny6uh4TSaTuOaaa0SvXr3EwYMH7f6OjUajbR/nH29nfwty6ehYq6urxUMPPSR27dolcnJyxObNm8WIESNE//79RX19vW0fvnJuO/t3LIQQer1eaLVa8dprr7W5D185r+7EcNNFr7zyiujdu7dQqVRizJgx4ocffrC9N3HiRDF79my79T/66CMxYMAAoVKpxEUXXSQ2bNjg4Yq7DkCbj7ffftu2zvnH+sADD9h+L3FxceLqq68W+/fv93zxTpgxY4ZISEgQKpVKJCUliRkzZogTJ07Y3veX89rsq6++EgBEdnZ2q/d8+bx+8803bf67bT4ei8UilixZIuLi4oRarRZXXnllq99BSkqKWLp0qd2yjv7m5dTR8ebk5LT7d/zNN9/Y9nH+8Xb2tyCXjo61rq5OTJ48WcTExIjAwECRkpIi7rnnnlYhxVfObWf/joUQ4vXXXxdBQUGisrKyzX34ynl1J0kIIdzaNERERETkQexzQ0RERH6F4YaIiIj8CsMNERER+RWGGyIiIvIrDDdERETkVxhuiIiIyK8w3BAREZFfYbghoh5n27ZtkCQJlZWVcpdCRG7AcENERER+heGGiIiI/ArDDRF5nMViQVZWFtLS0hAUFIRhw4Zh/fr1AM5dMtqwYQOGDh0KjUaDSy65BIcPH7bbx//93//hoosuglqtRmpqKlasWGH3vtFoxMKFC5GcnAy1Wo1+/frhrbfesltn3759GDVqFLRaLcaNG4fs7Gzbe4cOHcLll1+O0NBQhIWFYeTIkfjxxx/d9BshIldiuCEij8vKysK7776L1atX48iRI5g/fz5uv/12bN++3bbOww8/jBUrVmDv3r2IiYnBtGnT0NDQAMAaSm6++Wbccsst+Pnnn/HEE09gyZIlWLt2rW37WbNm4YMPPsDLL7+MY8eO4fXXX0dISIhdHY8++ihWrFiBH3/8EQEBAbjzzjtt7912223o1asX9u7di3379mHRokUIDAx07y+GiFxD7jt3ElHPUl9fL7Rardi5c6fd8rvuukvMnDnTdlfkDz/80PZeWVmZCAoKEuvWrRNCCHHrrbeKSZMm2W3/8MMPi0GDBgkhhMjOzhYAxKZNm9qsofkzNm/ebFu2YcMGAUAYDAYhhBChoaFi7dq13T9gIvI4ttwQkUedOHECdXV1mDRpEkJCQmyPd999FydPnrStN3bsWNvPkZGRGDhwII4dOwYAOHbsGMaPH2+33/Hjx+PXX3+F2WzGwYMHoVQqMXHixA5rGTp0qO3nhIQEAEBxcTEAYMGCBbj77ruRkZGBZ555xq42IvJuDDdE5FE1NTUAgA0bNuDgwYO2x9GjR239brorKCjIofVaXmaSJAmAtT8QADzxxBM4cuQIpk6diq1bt2LQoEH49NNPXVIfEbkXww0RedSgQYOgVquRm5uLfv362T2Sk5Nt6/3www+2nysqKnD8+HFceOGFAIALL7wQO3bssNvvjh07MGDAACiVSgwZMgQWi8WuD48zBgwYgPnz5+Prr7/G9ddfj7fffrtb+yMizwiQuwAi6llCQ0Px0EMPYf78+bBYLJgwYQL0ej127NiBsLAwpKSkAACefPJJREVFIS4uDo8++iiio6Mxffp0AMCDDz6I0aNH46mnnsKMGTOwa9curFq1Cv/4xz8AAKmpqZg9ezbuvPNOvPzyyxg2bBhOnz6N4uJi3HzzzZ3WaDAY8PDDD+PGG29EWloa8vPzsXfvXtxwww1u+70QkQvJ3emHiHoei8UiVq5cKQYOHCgCAwNFTEyMyMzMFNu3b7d19v3iiy/ERRddJFQqlRgzZow4dOiQ3T7Wr18vBg0aJAIDA0Xv3r3F888/b/e+wWAQ8+fPFwkJCUKlUol+/fqJNWvWCCHOdSiuqKiwrX/gwAEBQOTk5Aij0ShuueUWkZycLFQqlUhMTBT33XefrbMxEXk3SQghZM5XREQ227Ztw+WXX46KigqEh4fLXQ4R+SD2uSEiIiK/wnBDREREfoWXpYiIiMivsOWGiIiI/ArDDREREfkVhhsiIiLyKww3RERE5FcYboiIiMivMNwQERGRX2G4ISIiIr/CcENERER+heGGiIiI/Mr/B8gH4Fsj3XQ9AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}
